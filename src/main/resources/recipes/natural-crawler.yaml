config:
  fresh: true
  transformDefaultParams:
    jolt:
      - "${#recipe['templates']['joltRemoveEmptyMaps']}"
    neo4j:
      - "${@Utils.getEnvVariable('NEO4J_API_URI')}"
      - "${@Utils.createBasicAuthHeader(@Utils.getEnvVariable('NEO4J_USERNAME'), @Utils.getEnvVariable('NEO4J_PASSWORD'))}"
  options:
    - name: clearDatabase
      type: BOOLEAN
      label: "Clear database before anything?"
      defaultValue: false
executor: ProjectModelExecutor3.java
projectModel:
  _clearDatabase: "${#recipe['templates']['clearDatabase']}"
  filesTree.json: "${#recipe['templates']['filesTree']}"
  ddms: "$${@Utils.createWithAListOfKeys(#mapOfFiles.keySet().?[#this.endsWith('.ddm')], #recipe['templates']['ddm'])}"
  maps: "$${@Utils.createWithAListOfKeys(#mapOfFiles.keySet().?[#this.endsWith('.map')], #recipe['templates']['map'])}"
  programs: "$${@Utils.createWithAListOfKeys(#mapOfFiles.keySet().?[#this.endsWith('.adb')], #recipe['templates']['program'])}"
  persist.json: "$${#recipe['templates']['persist']}"
#  "${#mapOfFiles.keySet().?[#this.endsWith('.adb')]}":
#    "nodes.json": "${#recipe['templates']['program']}"
#macros:
#  neo4j: |-
#    @@@graph
templates:
  persist: |-
    @@@get("graph")
    @@@neo4j
    @@@jsonify
  joltRemoveEmptyMaps: |-
    [
      {
        "operation": "shift",
        "spec": {
          "locals|parameters": {
            "*": "&1.&0",
            "null": null
          },
          "subroutines": {
            "*": {
              "*": "&2[&1].&0",
              "locals|parameters": {
                "*": "&3[&2].&1.&0",
                "null": null
              }
            }
          },
          "*": "&0"
        }
      }
    ]
  clearDatabase: |-
    @@@_spel("${#projectContext.put('graph', @Utils.convertToConcurrent({'nodes': {}, 'relationships': {}}))}")
    @@@skip("${!#$api['configs']['options']['clearDatabase']}")
    @@@default("[]")
    @@@log("Starting to smash the database!")
    @@@neo4j
    @@@log("Successfully cleared the database!")
    @@@jsonify
    MATCH (n) DETACH DELETE n
  filesTree: |-
    @@@freemarker
    @@@spel("${@FileUtils.zipToMapOfStrings(#content, true)}")
    @@@set("mapOfFiles")
    @@@jsonify
    ${$api['files']['naturalProgramTest.zip']}
  ddm: |-
    @@@default("[]")
    @@@log("${#filePath.substring(5).replace('\\', '/')}")
    @@@freemarker
    @@@decodeBase64
    @@@parse("NATURAL")
    @@@objectify("${#recipe['models']['NaturalDataDefinitionModel']}")
    @@@jolt
    @@@nodify
    @@@neo4j
    @@@jsonify
    ${mapOfFiles[filePath?substring(5)?replace('\\', '/')]}
  map: |-
    @@@default("[]")
    @@@log("${#filePath.substring(5).replace('\\', '/')}")
    @@@freemarker
    @@@decodeBase64
    @@@parse("NATURAL")
    @@@objectify("${#recipe['models']['NaturalMap']}")
    @@@jolt
    @@@nodify
    @@@neo4j
    @@@jsonify
    ${mapOfFiles[filePath?substring(5)?replace('\\', '/')]}
  program: |- # parse end visit all callnatStatement
    @@@default("[]")
    @@@log("${#filePath.substring(9).replace('\\', '/')}")
    @@@freemarker
    @@@decodeBase64
    @@@parse("NATURAL", "callnatStatement", "performStatement", "findSqlStatement", "readSqlStatement", "storeStatement", "updateStatement", "inputStatement")
    @@@objectify("${#recipe['models']['NaturalProgram']}")
    @@@jolt
    @@@nodify
    @@@_spel("${#graph['nodes'].addAll(#content['nodes'])}")
    @@@_spel("${#graph['relationships'].addAll(#content['relationships'])}")
    @@@jsonify
    ${mapOfFiles[filePath?substring(9)?replace('\\', '/')]}
models:
  # Natural doesn't has divisions, sections, paragraphs, etc. So we will map everything directly to NaturalProgram
  # The "function definitions" are called SUBROUTINES in Natural, and they are defined using the SUBROUTINE statement
  # Files are directly mapped to JCL Step (which called the current program) based on the index of it... example:
  # //STEP1     EXEC PGM=NATURAL
  # //CMWKF01   DD  DSN=MY.INPUT.A,DISP=SHR
  # //          DD  DSN=MY.INPUT.B,DISP=SHR
  # //          DD  DSN=MY.INPUT.C,DISP=SHR
  # //CMWKF02   DD  DSN=MY.INPUT.D,DISP=SHR
  # A B C will be READ sequentially using:
  # READ WORK FILE 1 #LINE
  # /* processes A then B then C sequentially */
  # END-WORK
  # To read the D file, we reference the second DD statement of the same step: READ WORK FILE 2 #LINE...
  # Global Variables are declared at the beginning of the PROGRAM, before any executable statement
  # Local Variables are declared at the beginning of a SUBROUTINE, before any executable statement
  # On reporting mode, variables are implicitly declared when first used
  # /* DEFINE DATA optional here; we'll keep it simple
  # 1 #LINE   (A132)
  # 1 #COUNT  (I4)
  # /* Read both A then B as one logical stream
  # READ WORK FILE 1 #LINE
  # ADD 1 TO #COUNT
  # WRITE #LINE
  # END-WORK
  # WRITE / 'TOTAL RECORDS:' #COUNT
  # END
  # Screens are defined by MAP files, similar to BMS in COBOL.
  # Table structures are defined by DDM files, similar to DDL/DB2.
  NaturalComponent:
    filePath: "${#filePath}"
    rawCode: "${@ParserUtils.getContextRawText(#parent['meta']['ctx'])}"
    comments: null
    firstIndex: "${#parent['meta']['ctx'].getStart().getStartIndex()}"
    lastIndex: "${#parent['meta']['ctx'].getStop().getStopIndex()}"
    firstLine: "${#parent['meta']['ctx'].getStart().getLine()}"
    lastLine: "${#parent['meta']['ctx'].getStop().getLine()}"
  NaturalMap:
    meta:
      parsed: "${#content}"
      ctx: "${#self['parsed']['parserRuleContext']}"
    labels: ["NaturalMap", "Screen"]
    name: "${#fileNameWithoutExtension}"
    key: "${'nmap:' + #self['name']}"
    "": "${#recipe['models']['NaturalComponent']}"
  NaturalDataDefinitionModel:
    meta:
      parsed: "${#content}"
      ctx: "${#self['parsed']['parserRuleContext']}"
    "": "@@@script('NaturalCrawlerDefineDataStatement', 'LOCAL')"
  NaturalProgram:
    meta:
      parsed: "${#content}"
      ctx: "${#self['parsed']['parserRuleContext']}"
      visitedRules: "${#self['parsed']['visitedRules']}"
      mainPerforms: "${#self['parsed']['visitedRules']['performStatement'].?[@ParserUtils.matchFirstParent(#self['parsed']['parser'], #this, 'subroutineStatement') == null]}"
      subroutinePerforms: "${#self['parsed']['visitedRules']['performStatement'].?[@ParserUtils.matchFirstParent(#self['parsed']['parser'], #this, 'subroutineStatement') != null]}"
      mainStatements: "${#self['ctx'].naturalStatement().![#this.stmtScope()?.notMainCode()].?[#this != null]}"
      dataIndices: "${#self['ctx'].naturalStatement().![#this.stmtScope()?.notMainCode()?.defineDataStatement()].?[#this != null].![#this.getStop().getStopIndex()]}"
      subroutineIndices: "${#self['ctx'].naturalStatement().![#this.stmtScope()?.notMainCode()?.subroutineStatement()].?[#this != null].![#this.getStart().getStartIndex()]}"
    labels: ["NaturalProgram", "Program"]
    name: "${#fileNameWithoutExtension}"
    key: "${'np:' + #self['name']}"
    "": "${#recipe['models']['NaturalComponent']}"
    mode: "${#self['rawCode'].contains('DEFINE DATA') ? 'STRUCTURED' : 'REPORTING'}"
    parameters: "@@@script('NaturalCrawlerDefineDataStatement', 'PARAMETER')"
    locals: "@@@script('NaturalCrawlerDefineDataStatement', 'LOCAL')"
    mainCode: "${@ParserUtils.getContextRawText(#self['meta']['ctx'], #self['meta']['dataIndices'].isEmpty() ? 0 : T(java.util.Collections).max(#self['meta']['dataIndices']) + 1, #self['meta']['subroutineIndices'].isEmpty() ? null : T(java.util.Collections).min(#self['meta']['subroutineIndices']) - 1)}"
    "subroutines|${#self['meta']['ctx'].naturalStatement().![#this.stmtScope()?.notMainCode()].?[#this != null].![#this.subroutineStatement()].?[#this != null]}":
      meta:
        ctx: "${#item}"
      labels: ["NaturalSubroutine", "Function"]
      name: "${@Utils.flatten(#item.subroutineOptions().![@Utils.nvl(#this.SIMPLE_ID(), #this.ID())])[0].getText()}"
      key: "${#parent['key'] + '|sr:' + #self['name']}"
      "": "${#recipe['models']['NaturalComponent']}"
      parameters: "@@@script('NaturalCrawlerDefineDataStatement', 'PARAMETER')"
      locals: "@@@script('NaturalCrawlerDefineDataStatement', 'LOCAL')"
      "performs|${#self['parent']['meta']['subroutinePerforms'] == null ? {} : #self['parent']['meta']['subroutinePerforms'].?[@Utils.flatten(@ParserUtils.matchFirstParent(#self['parent']['meta']['parsed']['parser'], #this, 'subroutineStatement').subroutineOptions().![@Utils.nvl(#this.SIMPLE_ID(), #this.ID())])[0].getText() == #self['name']]}":
        meta:
          ctx: "${#item}"
        labels: [ "NaturalPerform" ]
        name: "${#item.performId().getText().replace('''', '').replace('\"', '')}"
        key: "${#parent['key'] + '|performIdx:' + #index + '|perform:' + #self['name']}"
        "": "${#recipe['models']['NaturalComponent']}"
    "calls|${#self['meta']['visitedRules']['callnatStatement']}":
      meta:
        ctx: "${#item}"
      labels: [ "NaturalCall" ]
      name: "${#item.STRING().getText().replace('''', '').replace('\"', '')}"
      key: "${#parent['key'] + '|callIdx:' + #index + '|call:' + #self['name']}"
      "": "${#recipe['models']['NaturalComponent']}"
    "performs|${#self['meta']['mainPerforms']}":
      meta:
        ctx: "${#item}"
      labels: [ "NaturalPerform" ]
      name: "${#item.performId().getText().replace('''', '').replace('\"', '')}"
      key: "${#parent['key'] + '|performIdx:' + #index + '|perform:' + #self['name']}"
      "": "${#recipe['models']['NaturalComponent']}"
    "maps|${#self['meta']['visitedRules']['inputStatement'].?[@Utils.nvl(#this.mapOption()?.ID(), #this.mapOption()?.STRING()) != null]}":
      meta:
        ctx: "${#item}"
      labels: [ "NaturalInputStatement" ]
      name: "${@Utils.nvl(#item.mapOption()?.ID(), #item.mapOption()?.STRING()).getText().replace('''', '').replace('\"', '')}"
      key: "${#parent['key'] + '|inputIdx:' + #index + '|input:' + #self['name']}"
      "": "${#recipe['models']['NaturalComponent']}"
    #"adabas|${#self['meta']['visitedRules']['findSqlStatement'].?[@ParserUtils.matchFirstParent(#self['meta']['parsed']['parser'], #this, 'subroutineStatement') == null]}|${#self['meta']['visitedRules']['readSqlStatement'].?[@ParserUtils.matchFirstParent(#self['meta']['parsed']['parser'], #this, 'subroutineStatement') == null]}":
    "adabasRead|${#self['meta']['visitedRules']['findSqlStatement']}|${#self['meta']['visitedRules']['readSqlStatement']}":
      meta:
        ctx: "${#item}"
      labels: [ "NaturalAdabasStatement" ]
      tag: "${#item.ID()?.getText() ?: '<FAILED>'}"
      name: "${#item.readOptions().?[#this.ID() != null][0].ID().getText()}"
      key: "${#parent['key'] + '|adabasReadIdx:' + #index +  '|adabasRead:' + #self['name']}"
      "": "${#recipe['models']['NaturalComponent']}"
    "adabasWrite|${#self['meta']['visitedRules']['storeStatement']}|${#self['meta']['visitedRules']['updateStatement']}":
      meta:
        ctx: "${#item}"
      labels: [ "NaturalAdabasStatement" ]
      #tag: "${#item.ID()?.getText() ?: '<FAILED>'}"
      name: "${(#item.ID() instanceof T(java.util.List) ? #item.ID()[0].getText() : #item.ID().getText()).replace('##L6490.', 'PES-PONTOS').replace('##L0780.', 'PES-ESTAGIARIO').replace('##L1220.', 'PES-ESTAGIARIO')}"
      key: "${#parent['key'] + '|adabasWriteIdx:' + #index +  '|adabasWrite:' + #self['name']}"
      "": "${#recipe['models']['NaturalComponent']}"
    "relationships|${#self['calls']}|${#self['adabasRead']}|${#self['adabasWrite']}|${#self['maps']}":
      label: "${#item['labels'][0] == 'NaturalCall' ? 'DEPENDS_ON' : (#item['labels'][0] == 'NaturalAdabasStatement' ? 'SQL' : 'SCREEN')}"
      startKey: "${#item['labels'][0] == 'NaturalCall' ? null : #item['key']}"
      endKey: "${#item['labels'][0] == 'NaturalCall' ? 'np:' + #item['name'] : (#item['labels'][0] == 'NaturalAdabasStatement' ? 'nddm:' + #item['name'] + '|dataArea:LOCAL' : 'nmap:' + #item['name'])}"
scripts:
  NaturalCrawlerDefineDataStatement:
    reference: NaturalCrawlerDefineDataStatement.groovy # resources/groovy as default
    name: NaturalCrawlerDefineDataStatement
    type: GROOVY
    body: |-
      import antlr4.AdabasNaturalParser
      import com.capco.brsp.synthesisengine.service.IExecutor
      import com.capco.brsp.synthesisengine.service.ScriptService
      import com.capco.brsp.synthesisengine.service.ScriptService2
      import com.capco.brsp.synthesisengine.service.SuperService
      import com.capco.brsp.synthesisengine.service.SuperService2
      import com.capco.brsp.synthesisengine.utils.ParserUtils
      import com.capco.brsp.synthesisengine.utils.SuperUtils
      import org.antlr.v4.runtime.ParserRuleContext
      import org.springframework.context.ApplicationContext
      
      class NaturalCrawlerDefineDataStatement implements IExecutor {
          SuperService2 superService = null
          ScriptService2 scriptService = null
          SuperUtils superUtils = SuperUtils.getInstance()
      
          class DefineDataAndFields {
              DefineDataAndFields(AdabasNaturalParser.DefineDataStatementContext dds, List<AdabasNaturalParser.FieldDefinitionContext> fdcList) {
                  this.defineDataStatementContext = dds
                  this.fields = fdcList
              }
      
              AdabasNaturalParser.DefineDataStatementContext defineDataStatementContext;
              List<AdabasNaturalParser.FieldDefinitionContext> fields = []
          }
      
          @Override
          Object execute(ApplicationContext applicationContext, Map<String, Object> projectContext, Object... params) {
              this.superService = applicationContext.getBean(SuperService2.class)
              this.scriptService = applicationContext.getBean(ScriptService2.class)
      
              def self = projectContext.self as Map<String, Object>
              def meta = self.meta as Map<String, Object>
              def ctx = meta.ctx as ParserRuleContext
      
              def naturalStatements = [] as List<AdabasNaturalParser.NaturalStatementContext>
              if (ctx instanceof AdabasNaturalParser.ProgramContext) {
                  naturalStatements = ctx.naturalStatement() ?: [] as List<AdabasNaturalParser.NaturalStatementContext>
              } else if (ctx instanceof AdabasNaturalParser.SubroutineStatementContext) {
                  naturalStatements = ctx.naturalStatement() ?: [] as List<AdabasNaturalParser.NaturalStatementContext>
              } else {
                  println "Unexpected context type: ${ctx.class.name}. Expected ProgramContext or SubroutineStatementContext."
              }
      
              def fieldsPerArea = naturalStatements
                      .collect { it.stmtScope()?.notMainCode()?.defineDataStatement() }
                      .findAll { dds -> dds && dds.defineDataOptions()?.any { opt -> opt.dataArea() } }
                      .groupBy { dds ->
                          dds.defineDataOptions()
                                  .collectMany { opt -> opt.dataArea() ?: [] }
                                  .first()
                                  .getText()
                                  .trim()
                                  .toUpperCase()
                      }
                      .collectEntries { area, ddss ->
                          def fields = ddss.collect { dds ->
                              new DefineDataAndFields(dds, dds.defineDataOptions().collectMany { opt -> opt.fieldDefinition() ?: [] })
                          }
      
                          if (fields.size() > 1) {
                              throw new IllegalStateException(
                                      "Multiple Define Data statements found for area '${area}'. " +
                                              "This executor expects only one Define Data statement per area.")
                          }
      
                          if (!fields || fields.isEmpty()) {
                              return null
                          }
      
                          [(area): fields.first()]
                      } as Map<String, DefineDataAndFields>
      
              def dataArea = params.first() as String
      
              def defineDataAndFields = fieldsPerArea[dataArea]
              if (!defineDataAndFields) {
                  return null
              }
      
              def filePath = (projectContext.filePath ?: "<UNKNOWN>") as String
              def fileNameWithoutExtension = (projectContext.fileNameWithoutExtension ?: "<UNKNOWN>") as String
      
              def dataAreaKey = "${self.key ?: 'nddm:' + fileNameWithoutExtension}|dataArea:${dataArea}".toString()
              def items = processNaturalDataItems(filePath, [dataAreaKey], defineDataAndFields)
      
              def dataAreaName = "NaturalDefineData${self.key == null ? 'Module' : dataArea.toLowerCase().capitalize()}".toString()
              return [
                  labels: [dataAreaName],
                  key: dataAreaKey,
                  name: dataAreaName,
                  items: items,
                  filePath: filePath,
                  rawCode: ParserUtils.getContextRawText(defineDataAndFields.getDefineDataStatementContext()),
                  firstIndex: defineDataAndFields.getDefineDataStatementContext().getStart().getStartIndex(),
                  lastIndex: defineDataAndFields.getDefineDataStatementContext().getStop().getStopIndex(),
                  firstLine: defineDataAndFields.getDefineDataStatementContext().getStart().getLine(),
                  lastLine: defineDataAndFields.getDefineDataStatementContext().getStop().getLine()
              ]
          }
      
          static List<Map<String, Object>> processNaturalDataItems(
                  String filePath,
                  List<String> keyPath,
                  DefineDataAndFields defineDataAndFields) {
      
              def parseLevel = { String s ->
                  if (!s) return 1
                  def m = (s =~ /\d+/)
                  m.find() ? (m.group(0) as int) : 1
              }
      
              // helper: find an existing sibling variable (same parent, same level) by name
              def findSiblingAtLevel = { List<Map> rootList, Map parentNode, int level, String varName ->
                  def siblings = (level == 1) ? rootList : (parentNode?.children ?: [])
                  siblings?.find { it.name?.equalsIgnoreCase(varName) && (it.labels ?: []).contains('NaturalVariable') }
              }
      
              def root = []
              def levelStack = [:] // Map<Integer, Map> â€” level -> node
              def keyStack = []    // Mirrors the nesting path of keys
      
              defineDataAndFields.getFields().each { fd ->
                  def rawCode   = ParserUtils.getContextRawText(fd)
                  def level     = parseLevel(fd?.fieldNumber()?.getText())
                  def isRedef   = (fd?.REDEFINE() != null)
                  def idTokens  = fd?.fieldIdentifier() ?: []
                  def name      = idTokens ? idTokens[0]?.ID()?.getText() as String : null
                  def dataType  = fd?.dataType()?.dataTypeSpecifier()?.ID()?[0]?.getText() as String
      
                  while (keyStack.size() >= level) {
                      keyStack.pop()
                  }
      
                  def parent = levelStack[level - 1]
                  def parentKey = ([*keyPath, *keyStack]).join('|')
      
                  if (isRedef) {
                      // This is a REDEFINE group node. It reuses storage of "name" (the target).
                      // It becomes the parent for subsequent (level+1) fields.
                      def redefineKey = parentKey + '|redefOf:' + (name ?: '<UNKNOWN>')
                      // try to link back to the target variable (if it already appeared)
                      def targetSibling = findSiblingAtLevel(root, parent, level, name ?: '')
                      def node = [
                              labels          : ['NaturalRedefine'],
                              filePath        : filePath,
                              rawCode         : rawCode,
                              level           : level,
                              // keep "name" only as the target being redefined; the group itself has no new identifier
                              name            : name,
                              redefineOf      : name,
                              redefineTargetKey: targetSibling?.key,  // null if not found (e.g., REDEFINE before target appears)
                              key             : redefineKey,
                              dataType        : 'REDEFINE',
                              firstIndex      : fd.getStart().getStartIndex(),
                              lastIndex       : fd.getStop().getStopIndex(),
                              firstLine       : fd.getStart().getLine(),
                              lastLine        : fd.getStop().getLine(),
                              children        : []
                      ]
      
                      if (level == 1) {
                          root << node
                      } else {
                          if (parent) {
                              parent.children << node
                          } else {
                              // malformed structure; best-effort: attach to root
                              root << node
                          }
                      }
      
                      levelStack[level] = node
                      // push a distinct key segment to avoid clashing with 'nvar:<name>'
                      keyStack << 'redefOf:' + (name ?: '<UNKNOWN>')
                      return // done for this token
                  }
      
                  // Regular variable node (not a REDEFINE clause)
                  def fullKey = parentKey + '|nvar:' + (name ?: '<UNKNOWN>')
      
                  def node = [
                          labels    : ['NaturalVariable'],
                          filePath  : filePath,
                          rawCode   : rawCode,
                          level     : level,
                          name      : name,
                          key       : fullKey,
                          dataType  : dataType,
                          firstIndex: fd.getStart().getStartIndex(),
                          lastIndex : fd.getStop().getStopIndex(),
                          firstLine : fd.getStart().getLine(),
                          lastLine  : fd.getStop().getLine(),
                          children  : []
                  ]
      
                  if (level == 1) {
                      root << node
                  } else {
                      if (parent) {
                          parent.children << node
                      } else {
                          // malformed structure; best-effort: attach to root
                          root << node
                      }
                  }
      
                  levelStack[level] = node
                  keyStack << 'nvar:' + (name ?: '<UNKNOWN>')
              }
      
              return root
          }
      }