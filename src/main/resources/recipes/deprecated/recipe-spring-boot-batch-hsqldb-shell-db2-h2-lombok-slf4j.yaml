executor: |-
    import com.capco.brsp.synthesisengine.dto.TaskMap
    import com.capco.brsp.synthesisengine.dto.TransformDto
    import com.capco.brsp.synthesisengine.service.ContextService
    import com.capco.brsp.synthesisengine.service.IExecutor
    import com.capco.brsp.synthesisengine.service.SuperService
    import com.capco.brsp.synthesisengine.service.ScriptService
    import com.capco.brsp.synthesisengine.utils.ConcurrentLinkedHashMap
    import com.capco.brsp.synthesisengine.utils.ConcurrentLinkedList
    import com.capco.brsp.synthesisengine.service.originalSS
    import com.capco.brsp.synthesisengine.utils.FileUtils
    import com.capco.brsp.synthesisengine.utils.JsonUtils
    import com.capco.brsp.synthesisengine.utils.OldUtils
    import com.capco.brsp.synthesisengine.utils.Utils
    import com.capco.brsp.synthesisengine.utils.YamlUtils
    import org.springframework.context.ApplicationContext
    import org.springframework.web.client.HttpServerErrorException
    import org.springframework.expression.BeanResolver
    import org.springframework.aop.framework.Advised
    import java.lang.reflect.Constructor
    import java.util.Arrays
    import org.springframework.aop.framework.ProxyFactory
    import org.aopalliance.intercept.MethodInterceptor

    import java.nio.file.Paths

    class CobolToJavaSpringMvcMicroservice implements IExecutor {
        String normalizeJavaIdentifier(String name) {
            return name.findAll(/[a-zA-Z0-9]+/).collect { dto -> dto.toLowerCase().capitalize() }.join('')
        }

        void prepare(ApplicationContext applicationContext, Map<String, Object> projectContext) {
            def recipe = projectContext.recipe as Map<String, Object>

            Map<String, Object> microserviceBaseModel = new ConcurrentLinkedHashMap<>()
            YamlUtils.traverse(microserviceBaseModel, recipe.microserviceModel, "")

            def tempCobolPrograms = projectContext['$api'].files.cobolPrograms
            def tempOriginalClustersContent = projectContext['$api'].files.clusters
            
            def cobolPrograms = (tempCobolPrograms instanceof Map ? tempCobolPrograms : JsonUtils.readAsMap(tempCobolPrograms)).cobolPrograms
            def originalClustersContent = tempOriginalClustersContent instanceof List ? tempOriginalClustersContent : JsonUtils.readAsList(tempOriginalClustersContent)

            List<Map<String, Object>> microservices = new ConcurrentLinkedList<>()
            projectContext.put("clusters", microservices)
            originalClustersContent.eachWithIndex { def report, int reportIndex ->
                projectContext.put("clusterIndex", reportIndex)

                def programsNames = report.paragraph.collect { paragraph -> ((String) paragraph.name).split('\\.')[0] }.unique()
                def matchedCobolPrograms = cobolPrograms.findAll(cob -> programsNames.contains(cob.name)) as List<Map<String, Object>>

                Map<String, Object> cluster = new ConcurrentLinkedHashMap<>()
                microservices.add(cluster)
                projectContext.put('cluster', cluster)

                String clusterName = report.cluster_name
                cluster.put("clusterName", clusterName)
                cluster.put("clusterNormalizedName", clusterName.replaceAll("\\s+-\\s+|\\s", "_").toLowerCase())
                cluster.put('programsNames', Utils.convertToConcurrent(programsNames))
                cluster.put('programsSources', Utils.convertToConcurrent(matchedCobolPrograms))
                cluster.put('report', Utils.convertToConcurrent(report))
                cluster.put('index', reportIndex)

                //def dtosNames = report.domain.findAll { it.startsWith("variable:") || it.startsWith("call-variable:") }.collect { it.replaceFirst("^.*variable:", "") }
                //def dtos = dtosNames.collect { storageName ->
                //    def storage = matchedCobolPrograms.collect { prog ->
                //        [program: prog.name, raw_code: prog.working_storage.data_entries.find { d -> d.name == storageName }?.raw_code]
                //    }.find { it != null && it.raw_code != null } ?: [program: '<NOT_FOUND>', raw_code: '<NOT_FOUND>']
                //    return [program: storage.program, name: storageName, code: storage.raw_code, file: normalizeJavaIdentifier(storageName) + 'Dto.java']
                //}
                //def dtosFiles = dtos.collect { it.file }
                
                def categorizeAndCollectNames = { domain, prefixes ->
                    domain.findAll { entry ->
                        prefixes.any { prefix -> entry.startsWith(prefix + ":") }
                    }.collect { entry ->
                        def matchingPrefix = prefixes.find { prefix -> entry.startsWith(prefix + ":") }
                        entry.replaceFirst("^.*${matchingPrefix}:", "")
                    }
                }
                
                def getStorageEntry = { section, storageName, sourcePrograms -> 
                    sourcePrograms.collect { prog ->
                        [program: prog.name, raw_code: prog[section].data_entries.find { d -> d.name == storageName }.raw_code]
                    }.find { it != null && it.raw_code != null } ?: [program: '<NOT_FOUND>', raw_code: '<NOT_FOUND>']
                }
                
                def generateStructure = { section, prefixList, fileSuffix -> 
                    def names = categorizeAndCollectNames(report.domain, prefixList)
                    names.collect { storageName ->
                        def storage = getStorageEntry(section, storageName, matchedCobolPrograms)
                        [program: storage.program, name: storageName, code: storage.raw_code, file: normalizeJavaIdentifier(storageName) + fileSuffix]
                    }
                }
                
                def dtosNames = categorizeAndCollectNames(report.domain, ["call-variable"])
                def filesNames = categorizeAndCollectNames(report.domain, ["file"])
                def modelsNames = categorizeAndCollectNames(report.domain, ["table", "cursor"])
                def localsNames = categorizeAndCollectNames(report.domain, ["variable"])
                
                def dtos = generateStructure('working_storage', ["call-variable"], "Dto.java")
                def files = generateStructure('file_section', ["file"], "File.java")
                def models = generateStructure('working_storage', ["table", "cursor"], ".java")
                def locals = generateStructure('working_storage', ["variable"], null)
                
                def dtosFiles = dtos.collect { it.file }
                def filesFiles = files.collect { it.file }
                def modelsFiles = models.collect { it.file }
                
                def paragraphs = report.paragraph.collect { it ->
                    String programName = ((String) it.name).split("\\.")[0]
                    String paragraph = ((String) it.name).split("\\.")[1]
                    def paragraphCode = matchedCobolPrograms.collect { prog ->
                        prog.procedure_division.paragraphs.find { p -> p.name == paragraph }?.raw_code
                    }.find { it != null } ?: '<NOT_FOUND>'

                    def children = it.children.collect { entry ->
                        def parts = entry.split(/\./)
                        return [
                                program  : parts[0],
                                paragraph: parts[1]
                        ]
                    }

                    return [program: programName, paragraph: paragraph, code: paragraphCode, children: children]
                }

                cluster.put('dtos', Utils.convertToConcurrent(dtos))
                cluster.put('dtosFiles', Utils.convertToConcurrent(dtosFiles))
                cluster.put('files', Utils.convertToConcurrent(files))
                cluster.put('filesFiles', Utils.convertToConcurrent(filesFiles))
                cluster.put('models', Utils.convertToConcurrent(models))
                cluster.put('modelsFiles', Utils.convertToConcurrent(modelsFiles))
                cluster.put('locals', Utils.convertToConcurrent(locals))
                
                cluster.put('paragraphs', Utils.convertToConcurrent(paragraphs))

                Map<String, Object> clusterModel = expand(applicationContext, projectContext, new ConcurrentLinkedHashMap<>(microserviceBaseModel), cluster)
                cluster.put('clusterModel', Utils.convertToConcurrent(clusterModel))

                projectContext.remove('cluster')
                projectContext.remove('clusterIndex')
            }

            def modules = microservices.collect {
                "<module>${it.clusterNormalizedName}</module>".toString()
            }.join("\n")

            def superpom = """
                    <project xmlns="http://maven.apache.org/POM/4.0.0"
                            xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
                            xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
                    <modelVersion>4.0.0</modelVersion>
                    
                    <groupId>${recipe.vars.groupId}</groupId>
                    <artifactId>coboltojava</artifactId>
                    <version>${recipe.vars.appVersion}</version>
                    <packaging>pom</packaging>
        
                        <properties>              
                            <java.version>21</java.version>
                            <quarkus.version>3.16.2</quarkus.version>
                            <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
                            
                            <maven.compiler.source>\${java.version}</maven.compiler.source>
                            <maven.compiler.target>\${java.version}</maven.compiler.target>
                            
                        </properties>
                    
                    <modules>
                        ${modules}
                    </modules>

                        
                        <dependencyManagement>
                            <dependencies>
                                <dependency>
                                    <groupId>io.quarkus.platform</groupId>
                                    <artifactId>quarkus-bom</artifactId>
                                    <version>\${quarkus.version}</version>
                                    <type>pom</type>
                                    <scope>import</scope>
                                </dependency>
                            </dependencies>
                        </dependencyManagement>
                        
                        <build>
                            <plugins>
                                <plugin>
                                    <groupId>\${quarkus.platform.group-id}</groupId>
                                    <artifactId>quarkus-maven-plugin</artifactId>
                                    <version>\${quarkus.platform.version}</version>
                                    <extensions>true</extensions>
                                    <executions>
                                        <execution>
                                            <goals>
                                                <goal>build</goal>
                                                <goal>generate-code</goal>
                                                <goal>generate-code-tests</goal>
                                                <goal>native-image-agent</goal>
                                            </goals>
                                        </execution>
                                    </executions>
                                </plugin>
                                <plugin>
                                    <artifactId>maven-compiler-plugin</artifactId>
                                    <version>\${compiler-plugin.version}</version>
                                    <configuration>
                                        <parameters>true</parameters>
                                    </configuration>
                                </plugin>
                            </plugins>
                        </build>
                    
                    </project>
                    """.toString()

            def superpomPath = FileUtils.absolutePathJoin(projectContext.rootFolder, 'pom.xml')
            FileUtils.writeFile(superpomPath, superpom, false)
        }

        @Override
        List<TaskMap> prepareListOfTasks(ApplicationContext applicationContext, UUID projectUUID) {
            def originalSS = applicationContext.getBean(SuperService.class)
            def scriptService = applicationContext.getBean(ScriptService.class)
            def contextService = applicationContext.getBean(ContextService.class)
            def projectContext = contextService.getProjectContext()

            String rootFolder = FileUtils.absolutePathJoin(FileUtils.USER_TEMP_PROJECTS_FOLDER_PATH, projectUUID)
            projectContext.put('rootFolder', rootFolder)

            Map<String, String> transforms = (projectContext.recipe?.transforms ?: new ConcurrentLinkedHashMap<>()) as Map<String, String>
            transforms.each {key, val ->
                scriptService.getGroovyTransform(key, val)
            }

            String beforeAllExpression = projectContext.recipe?.beforeAll
            processAction(scriptService, beforeAllExpression, null, null)

            prepare(applicationContext, projectContext)

            def clusters = (List<Map<String, Object>>) projectContext.clusters

            List<TaskMap> listOfTaskMap = new ConcurrentLinkedList<>()
            clusters.eachWithIndex { Map<String, Object> cluster, int index ->
                Map<String, Object> clusterModel = cluster.clusterModel as Map<String, Object>

                clusterModel.eachWithIndex { fileEntry, fileIndex ->
                    {
                        def filePath = fileEntry.getKey()
                        def fileValue = fileEntry.getValue()
                        def fileFolder = Paths.get(filePath).getParent().toString()
                        def fileName = Paths.get(filePath).getFileName().toString()
                        def extensionIdx = fileName.lastIndexOf('.')
                        def extensionName = fileName.substring(extensionIdx + 1)
                        def fileNameWithoutExtension = extensionIdx > -1 ? fileName.substring(0, extensionIdx) : ""                        

                        Runnable task = {
                            contextService.setProjectUUID(projectUUID)
                            def context = contextService.getProjectContext()

                            context.put("cluster", cluster)
                            context.put('filePath', filePath)
                            context.put('fileFolder', fileFolder)
                            context.put('fileName', fileName)
                            context.put('extensionName', extensionName)
                            context.put('fileNameWithoutExtension', fileNameWithoutExtension)

                            if (fileIndex == 0) {
                                String beforeEachMicroserviceExpression = context.recipe?.beforeEachMicroservice
                                processAction(scriptService, beforeEachMicroserviceExpression, null, null)
                            }

                            var relativeFilePath = FileUtils.pathJoin(cluster.clusterNormalizedName, filePath)
                            var fullFilePath = FileUtils.absolutePathJoin(context.rootFolder, relativeFilePath)

                            List<TransformDto> history = new ConcurrentLinkedList<>()

                            String beforeEachFileExpression = context.recipe?.beforeEachFile
                            processAction(scriptService, beforeEachFileExpression, null, history)
                            
                            def attempts = 0
                            def result = "FAILED due to LLM connection problems!"
                            while (attemps++ < 3) {
                                try {
                                    result = scriptService.autoEval(fileValue as String, history) as String
                                    break
                                } catch (HttpServerErrorException ex) {
                                    System.out.println("HttpServerErrorException during the attempt ${attempts} of 3. Waiting 1 minute before retrying!".toString())
                                    Thread.sleep(60000)
                                }
                            }

                            Set<String> importLines = []
                            if (result != null && !result.isEmpty()) {
                                result.eachLine { line ->
                                    def trimmedLine = line.trim()
                                    if (trimmedLine.startsWith("import")) {
                                        importLines << line
                                    }
                                }
                                List<String> uniqueImportLines = new ConcurrentLinkedList<>(importLines)
                                def listOfImports = (cluster.get('imports') ?: []) as List<String>
                                listOfImports.addAll(uniqueImportLines)
                                cluster.put('imports', listOfImports)
                            }

                            String afterEachFileExpression = context.recipe?.afterEachFile
                            result = processAction(scriptService, afterEachFileExpression, result, history)

                            FileUtils.writeFile(fullFilePath, result ?: "", false)

                            projectContext.putIfAbsent("files_metadata", new ConcurrentLinkedHashMap<>())
                            def filesMetadata = projectContext.get("files_metadata") as Map<String, Object>
                            filesMetadata.put(relativeFilePath.toString(), [history: history])

                            context.remove('filePath')
                            context.remove('fileName')
                            context.remove('cluster')

                            if (fileIndex == clusterModel.size() - 1) {
                                String afterEachMicroserviceExpression = context.recipe?.afterEachMicroservice
                                processAction(scriptService, afterEachMicroserviceExpression, null, null)
                            }
                        }

                        String simplifiedFilePath = Utils.simplifyPath(filePath, "\\\\|/", ".")

                        def taskMap = new TaskMap("Microservice ${index} - ${simplifiedFilePath}".toString(), task)
                        listOfTaskMap.add(taskMap)
                    }
                }
            }

            return listOfTaskMap
        }

        String processAction(ScriptService scriptService, String action, String input, List<TransformDto> history) {
            if (action == null) {
                return input
            }

            String expression = action
            if (input != null) {
                expression += "\n" + input
            }
            
            def attempts = 0
            def result = "FAILED due to LLM connection problems!"
            while (attemps++ < 3) {
                try {
                    result = scriptService.autoEval(expression, history ?: new ConcurrentLinkedList<>() as List<TransformDto>)
                    break
                } catch (HttpServerErrorException ex) {
                    System.out.println("HttpServerErrorException during the attempt ${attempts} of 3. Waiting 1 minute before retrying!".toString())
                    Thread.sleep(60000)
                }
            }

            return result
        }

        Map<String, Object> expand(ApplicationContext applicationContext, Map<String, Object> projectContext, Map<String, Object> microserviceModel, Map<String, Object> cluster) {
            def scriptService = applicationContext.getBean(ScriptService.class)

            Map<String, Object> solved = new ConcurrentLinkedHashMap<>()
            microserviceModel.each { filePath, fileValue ->
                {
                    projectContext.put('filePath', filePath)
                    def fileName = Paths.get(filePath).getFileName().toString()
                    projectContext.put('fileName', fileName)

                    evaluate(scriptService, fileValue, filePath, solved)

                    projectContext.remove('filePath')
                    projectContext.remove('fileName')
                }
            }

            return solved
        }

        def evaluate(ScriptService scriptService, Object object, String key, Map<String, Object> targetMap) {
            key = scriptService.evalFreemarker(key)
        
            if (object instanceof String) {
                if (scriptService.isValidSpEL((String) object)) {
                    String expression = (String) object
                    try {
                        var result = scriptService.evalSpEL(expression)
                        if (result instanceof Map || result instanceof List) {
                            def subItems = new ConcurrentLinkedHashMap<>()
                            YamlUtils.traverse(subItems, result, key)
                            targetMap.putAll(subItems)
                        } else {
                            result = Utils.extractMarkdownCode(result)
                            targetMap.put(key, result)
                        }
                    } catch (Exception ignored) {
                        targetMap.put(key, null)
                    }
                } else {
                    var freemarkerResult = scriptService.evalFreemarker(object)
                    targetMap.put(key, freemarkerResult)
                }
            } else {
                targetMap.put(key, object)
            }
        }
    }
planner:
  data:
  test:
microserviceModel:
  src:
    main:
      resources/temp/stepsInputs.json: ${#recipe['prompts']['stepsInputs']}
      java:
        com:
          capco:
            ${cluster.clusterNormalizedName}:
              #fakes: "${#recipe['prompts']['fakeDiagram']}"
              model:
                Model.java: ${#recipe['prompts'}['model']}
              exception:
                ExceptionsHandler.java: ${#recipe['prompts']['exception']}
              service:
                MainService.java: "${#recipe['prompts']['service2']}"
                TaskletService.java: "${#recipe['prompts']['taskletService']}"
              config:
                DataSourceConfig.java: ${#recipe['prompts']['dataSourceConfig']}
                BatchConfig.java: ${#recipe['prompts']['batchConfig']}
                BatchStepsConfig2.java: ${#recipe['prompts']['batchStepsConfig2']}
              #controller:
              #  BatchJobController.java: "${#recipe['vars']['batchJobController']}"
              shell:
                ShellCommands.java: "${#recipe['prompts']['shellCommands']}"
              Application.java: ${#recipe['vars']['application']}
              diagrams:
                classDiagram.uml: ${#recipe['prompts']['classDiagram']}
                flowDiagram.uml: ${#recipe['prompts']['flowDiagram']}
      resources:
        application.yaml: ${#recipe['vars']['applicationYaml']}
  test:
    java:
      com:
        capco:
          ${cluster.clusterNormalizedName}:
            service:
              MainServiceTest.java: ${#recipe['prompts']['serviceTest']}
  .mvn:
    wrapper:
      maven-wrapper.properties: ${#recipe['vars']['mvnWrapperProperties']}
  .dockerignore: ${#recipe['vars']['dockerignore']}
  .gitattributes: ${#recipe['vars']['gitattributes']}
  .gitignore: ${#recipe['vars']['gitignore']}
  Dockerfile: ${#recipe['vars']['Dockerfile']}
  mvnw: ${#recipe['vars']['mvnw']}
  mvnw.cmd: ${#recipe['vars']['mvnwCmd']}
  pom.xml: ${#recipe['vars']['pom']}
  README.md: ${#recipe['vars']['readme']}
beforeAll: |-
    @@@groovy
    import com.capco.brsp.synthesisengine.service.IExecutor
    import com.capco.brsp.synthesisengine.utils.SuperUtils
    import com.capco.brsp.synthesisengine.utils.JsonUtils
    import org.springframework.context.ApplicationContext
    
    import java.util.List
    import java.util.Map

    class prepare implements IExecutor {
        Object execute(ApplicationContext applicationContext, Map<String, Object> projectContext) {
            def tempCobolPrograms = projectContext['$api'].files.cobolPrograms
            def tempOriginalClustersContent = projectContext['$api'].files.clusters
            def tempTables = projectContext['$api'].files.tables
            def tables = tempTables instanceof List ? tempTables : JsonUtils.readAsList(tempTables)
            def cobol = (tempCobolPrograms instanceof Map ? tempCobolPrograms : JsonUtils.readAsMap(tempCobolPrograms)).cobolPrograms
            def report = tempOriginalClustersContent instanceof List ? tempOriginalClustersContent : JsonUtils.readAsList(tempOriginalClustersContent)
            def jclProfile = JsonUtils.readAsList(projectContext.recipe.vars.jclProfile)
            projectContext.jclProfile = jclProfile

            def output = []
            def port = 8080

            report.eachWithIndex { cluster, index ->
                def programs = cluster.paragraph
                        .collect { paragraph -> ((String) paragraph.name).split('\\.')[0] }
                        .unique()
                def transformedCluster = [
                        name: cluster.cluster_name,
                        normalizedName: cluster.cluster_name.replaceAll("\\s+-\\s+|\\s", "_").toLowerCase(),
                        port: port + index,
                        dtos: cluster.domain
                                .findAll { it.startsWith("call-variable:") }
                                .collect { variable ->
                                    def storageName = variable.replaceFirst("^.*", "")
                                    def storage = cobol.findAll { programs.contains(it.name) }
                                            .collect { cob ->
                                                [
                                                        program: cob.name,
                                                        code: cob.working_storage.data_entries.find { d -> d.name == storageName }?.raw_code
                                                ]
                                            }
                                            .find { it != null && it.code != null } ?: [program: '<NOT_FOUND>', raw_code: '<NOT_FOUND>']
                                    return [program: storage.program, name: storageName, code: storage.code, file: storageName.findAll(/[a-zA-Z0-9]+/).collect { dto -> dto.toLowerCase().capitalize() }.join('') + 'Dto.java']
                                },
                        files: cluster.domain
                                .findAll { it.startsWith("file-") }
                                .collect { variable ->
                                    def storageName = variable.replaceFirst("^.*:", "")
                                    def storage = cobol.findAll { programs.contains(it.name) }
                                                    .collect { cob ->
                                                        [
                                                            program: cob.name,
                                                            code: cob.file_section?.data_entries.find { d -> d.name == storageName }?.raw_code
                                                        ]
                                                    }
                                                    .find { it != null && it.code != null } ?: [program: '<NOT_FOUND>', raw_code: "<NOT_FOUND>"]
                                    return [program: storage.program, name: storageName, code: storage.code, file: storageName.findAll(/[a-zA-Z0-9]+/).collect { dto -> dto.toLowerCase().capitalize() }.join('') + 'Model.java']
                                },
                        tables: cluster.domain
                                .findAll { it.startsWith("table-") }
                                .collect { variable ->
                                    def tableName = variable.replaceFirst("^.*:", "")
                                    def table = tables.find { t -> tableName.equalsIgnoreCase(t.table.name) }?.table
                                    return [program: "<NOT_FOUND>", name: table?.name, code: table?.raw_code, file: tableName.findAll(/[a-zA-Z0-9]+/).collect { dto -> dto.toLowerCase().capitalize() }.join('') + 'Model.java']
                                },
                        locals: cluster.domain
                                .findAll { !it.startsWith("call-variable") && !it.startsWith("file-") && !it.startsWith("table-") && !it.startsWith("cursor-") }
                                .collect { variable ->
                                    def storageName = variable.replaceFirst("^.*:", "")
                                    def storage = cobol.findAll { programs.contains(it.name) }
                                                    .collect { cob ->
                                                        [
                                                            program: cob.name,
                                                            code: cob.working_storage.data_entries.find { d -> d.name == storageName }?.raw_code
                                                        ]
                                                    }
                                                    .find { it != null && it.code != null } ?: [program: '<NOT_FOUND>', raw_code: "<NOT_FOUND>"]
                                    return [program: storage.program, name: storageName, code: storage.code, file: storageName.findAll(/[a-zA-Z0-9]+/).collect { dto -> dto.toLowerCase().capitalize() }.join('') + 'Dto.java']
                                },                        
                        paragraphs: cluster.paragraph.collect { para ->
                            def parts = para.name.split('\\.')
                            def calls = para.children.collect { child ->
                                def childParts = child.split('\\.')
                                def microservice = report.find { it.paragraph.any { p -> p.name == child } }.cluster_name
                                if (microservice == cluster.cluster_name) {
                                    return null
                                }
                                return [
                                        microservice: microservice,
                                        program: childParts[0],
                                        paragraph: childParts[1],
                                        code: cobol.findAll { it.name == childParts[0] }.collect { source ->
                                            source.procedure_division.paragraphs.find { p -> p.name == childParts[1] }?.raw_code
                                        }.find { it != null } ?: '<NOT_FOUND>',
                                        endpoint: "/${childParts[0].toLowerCase()}-${childParts[1].toLowerCase()}".toString()
                                ]
                            }.findAll { it != null }
                            [
                                    cobol_path: para.name,
                                    program: parts[0],
                                    name: parts[1],
                                    code: cobol.findAll { it.name == parts[0] }.collect { source ->
                                        source.procedure_division.paragraphs.find { p -> p.name == parts[1] }?.raw_code
                                    }.find { it != null } ?: '<NOT_FOUND>',
                                    calls: calls
                            ]
                        }
                ]
                def clientsMap = [:]
                transformedCluster.paragraphs.each { p ->
                    p.calls.each { entry ->
                        def microserviceClientName = entry.microservice.findAll(/[a-zA-Z0-9]+/).collect { dto -> dto.toLowerCase().capitalize() }.join('') + 'Client.java'
                        if (!clientsMap.containsKey(microserviceClientName)) {
                            clientsMap[microserviceClientName] = [(entry.endpoint): entry.code]
                        } else {
                            def microserviceClientMap = clientsMap[microserviceClientName]
                            if (!microserviceClientMap.containsKey(entry.endpoint)) {
                                microserviceClientMap[entry.endpoint] = entry.code
                            }
                        }
                    }
                }
                transformedCluster.clients = clientsMap
                output << transformedCluster
            }

            projectContext.put('blueprint', output)
            return output
        }
    }
beforeEachMicroservice: |-
  #@@@openllmthread
  #@@freemarker
  #@@@prompt
  #PAY ATENTION TO THE PROGRAMS BELOW!!! THEY ARE YOUR REFERENCE TO GENERATE ALL THE CONTENT ASKED FOR THE NEXT PROMPTS!!!
  #
  #<#list cluster.programsSources as _programSource>
  #  ########### START OF PROGRAM: ${_programSource.name}
  #  ${_programSource.raw_code}
  #  ########### END OF PROGRAM: ${_programSource.name}
  #</#list>
  #
  #Also, do a first analyse of the content to figure out which VARIABLES are used as intermediate values an which of them are supposed to be persisted/attached to a file/database.
afterEachFile: |-
  @@@utils.extractMarkdownCode
afterEachMicroservice: |-
  #@@@closellmthread
vars:
  jclProfileSchema: |-
    [
        {
            "jclName": "TESTJCL.jcl",
            "stepName": "STEP010",
            "execTarget": "IEFBR14",
            "isProgramCall": false,
            "parameters: [
                "",
                ""
            ]
        }
    ]
  groupId: com.capco
  appVersion: 1.0.0
  jdkVersion: 21
  springBootVersion: 3.3.4
  springShellVersion: 3.4.0
  port: 8080
  dtoExpression: "${#recipe['prompts']['dto']}"
  serviceExpression: "${#recipe['prompts']['service']}"
  controllerExpression: "${#recipe['prompts']['controller']}"
  controllerName: MainController
  application: |-
    @@@freemarker
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName};
    
    import org.springframework.boot.SpringApplication;
    import org.springframework.boot.autoconfigure.SpringBootApplication;
    import org.springframework.data.jpa.repository.EnableJpaRepositories;
    
    @SpringBootApplication
    @EnableJpaRepositories
    public class Application {
        public static void main(String[] args) {
            SpringApplication.run(Application.class, args);
        }
    }
  applicationYaml: |-
    @@@freemarker
    server:
      port: ${(recipe.vars.port + cluster.index)?string["0"]}
    spring:
      batch:
        jdbc:
          initialize-schema: always
      datasource:
        url: jdbc:h2:mem:test;DB_CLOSE_DELAY=-1;MODE=PostgreSQL
        driverClassName: org.h2.Driver
        username: sa
        password: password
      h2:
        console:
          enabled: true
          path: /h2-console
      jpa:
        database-platform: org.Hibernate.dialect.H2Dialect
        defer-datasource-initialization: true
        hibernate:
          ddl-auto: update
        properties:
          hibernate:
            format_sql: true
        show-sql: true
      shell:
        interactive:
          enabled: true
      sql:
        init:
          platform: h2
  mvnWrapperProperties: |-
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #   http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    wrapperVersion=3.3.2
    distributionType=only-script
    distributionUrl=https://repo.maven.apache.org/maven2/org/apache/maven/apache-maven/3.9.9/apache-maven-3.9.9-bin.zip
    distributionSha256Sum=4ec3f26fb1a692473aea0235c300bd20f0f9fe741947c82c1234cefd76ac3a3c

  gitattributes: |-
    /mvnw text eol=lf
    *.cmd text eol=crlf

  gitignore: |-
    HELP.md
    target/
    !.mvn/wrapper/maven-wrapper.jar
    !**/src/main/**/target/
    !**/src/test/**/target/
    
    ### STS ###
    .apt_generated
    .classpath
    .factorypath
    .project
    .settings
    .springBeans
    .sts4-cache
      
    ### IntelliJ IDEA ###
    .idea
    *.iws
    *.iml
    *.ipr
    
    ### NetBeans ###
    /nbproject/private/
    /nbbuild/
    /dist/
    /nbdist/
    /.nb-gradle/
    build/
    !**/src/main/**/build/
    !**/src/test/**/build/
    
    ### VS Code ###
    .vscode/
  Dockerfile: |-
    @@@freemarker
    FROM openjdk:${vars.jdkVersion}-jdk-alpine
    WORKDIR /app
    COPY target/${cluster.clusterNormalizedName + '_1.0.0.jar'} /app/api.jar
    EXPOSE ${Utils.integerGetFromIndexOrMaxIncOrDefaultAndSetTarget(recipe, 'vars.portsCache', cluster.index, recipe.vars.port)?string["0"]}
    ENTRYPOINT ["java", "-jar", "api.jar"]
  pom: |-
    @@@freemarker
    @@@newPrompt
    [CONTEXT]
    ${cluster.imports?join("\n")}
    [/CONTEXT]
    
    Using the pom.xml content below as a template, retrieve a complete version of the pom.xml content replacing the !!imports!! with the needed java dependencies for the actual context. DON'T INCLUDE ANYTHING ELSE, JUST THE POM.XML CONTENT!
    Also include the most recent Spring Sleuth dependency to improve the loggin system.
    Keep the dependencies already included as it is.
    
    <project xmlns="http://maven.apache.org/POM/4.0.0"
             xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
             xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
        <modelVersion>4.0.0</modelVersion>
    
        <parent>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-parent</artifactId>
            <version>${springBootVersion}</version>
            <relativePath/>
        </parent>
        
        <groupId>${recipe.vars.groupId}</groupId>
        <artifactId>${cluster.clusterNormalizedName}</artifactId>
        <version>${recipe.vars.appVersion}</version>
        <packaging>jar</packaging>
    
        <name>${cluster.clusterNormalizedName}</name>
        <description>${cluster.clusterNormalizedName} for Spring Boot</description>
    
        <properties>
            <java.version>${recipe.vars.jdkVersion}</java.version>
            <spring-boot.version>${recipe.vars.springBootVersion}</spring-boot.version>
            <spring-shell.version>${recipe.vars.springShellVersion}</spring-shell.version>
        </properties>
        
        <#noparse>
        <dependencyManagement>
            <dependencies>
                <dependency>
                    <groupId>org.springframework.boot</groupId>
                    <artifactId>spring-boot-dependencies</artifactId>
                    <version>${spring-boot.version}</version>
                    <type>pom</type>
                    <scope>import</scope>
                </dependency>
            </dependencies>
        </dependencyManagement>
        </#noparse>
    
        <dependencies>
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-starter-web</artifactId>
            </dependency>
            <dependency>
                <#noparse>
                <groupId>org.springframework.shell</groupId>
                <artifactId>spring-shell-starter</artifactId>
                <version>${spring-shell.version}</version>
                </#noparse>
            </dependency>
            <dependency>
                <groupId>org.projectlombok</groupId>
                <artifactId>lombok</artifactId>
                <scope>provided</provided>
            </dependency>
            <dependency>
                <groupId>com.h2databsase</groupId>
                <artifactId>h2</artifactId>
            </dependency>
            <dependency>
                <groupId>org.hsqldb</groupId>
                <artifactId>hsqldb</artifactId>
                <version>2.7.4</version>
                <scope>runtime</scope>
            </dependency>            
            !!imports!!
        </dependencies>
    
        <#noparse>
        <build>
            <plugins>
                <plugin>
                    <groupId>org.apache.maven.plugins</groupId>
                    <artifactId>maven-compiler-plugin</artifactId>
                    <version>3.13.0</version>
                    <configuration>
                        <source>${java.version}</source>
                        <target>${java.version}</target>
                    </configuration>
                </plugin>
                <plugin>
                    <groupId>org.springframework.boot</groupId>
                    <artifactId>spring-boot-maven-plugin</artifactId>
                    <version>3.4.1</version>
                </plugin>
            </plugins>
        </build>
        </#noparse>
    
    </project>
  readme: |-
    @@@freemarker
    # README #
    
    Microservice created based on ${recipe.vars.artifactId} Cobol Application
    
    ### Docker run local
    ```bash
    ./mvnw clean package
    docker-compose up -d
    ```
    
    ### Run Install dependencies
    ```bash
    ./mvnw clean package
    ```
    
    ### Run spring tests and package
    ```bash
    ./mvnw clean package
    ```
    
    ### Run spring local
    ```bash
    ./mvnw spring-boot:run
    ```
  mvnw: |-
    #!/bin/sh
    # ----------------------------------------------------------------------------
    # Licensed to the Apache Software Foundation (ASF) under one
    # or more contributor license agreements.  See the NOTICE file
    # distributed with this work for additional information
    # regarding copyright ownership.  The ASF licenses this file
    # to you under the Apache License, Version 2.0 (the
    # "License"); you may not use this file except in compliance
    # with the License.  You may obtain a copy of the License at
    #
    #    http://www.apache.org/licenses/LICENSE-2.0
    #
    # Unless required by applicable law or agreed to in writing,
    # software distributed under the License is distributed on an
    # "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    # KIND, either express or implied.  See the License for the
    # specific language governing permissions and limitations
    # under the License.
    # ----------------------------------------------------------------------------

    # ----------------------------------------------------------------------------
    # Apache Maven Wrapper startup batch script, version 3.3.2
    #
    # Optional ENV vars
    # -----------------
    #   JAVA_HOME - location of a JDK home dir, required when download maven via java source
    #   MVNW_REPOURL - repo url base for downloading maven distribution
    #   MVNW_USERNAME/MVNW_PASSWORD - user and password for downloading maven
    #   MVNW_VERBOSE - true: enable verbose log; debug: trace the mvnw script; others: silence the output
    # ----------------------------------------------------------------------------

    set -euf
    [ "${MVNW_VERBOSE-}" != debug ] || set -x

    # OS specific support.
    native_path() { printf %s\\n "$1"; }
    case "$(uname)" in
    CYGWIN* | MINGW*)
      [ -z "${JAVA_HOME-}" ] || JAVA_HOME="$(cygpath --unix "$JAVA_HOME")"
      native_path() { cygpath --path --windows "$1"; }
      ;;
    esac

    # set JAVACMD and JAVACCMD
    set_java_home() {
      # For Cygwin and MinGW, ensure paths are in Unix format before anything is touched
      if [ -n "${JAVA_HOME-}" ]; then
        if [ -x "$JAVA_HOME/jre/sh/java" ]; then
          # IBM's JDK on AIX uses strange locations for the executables
          JAVACMD="$JAVA_HOME/jre/sh/java"
          JAVACCMD="$JAVA_HOME/jre/sh/javac"
        else
          JAVACMD="$JAVA_HOME/bin/java"
          JAVACCMD="$JAVA_HOME/bin/javac"

          if [ ! -x "$JAVACMD" ] || [ ! -x "$JAVACCMD" ]; then
            echo "The JAVA_HOME environment variable is not defined correctly, so mvnw cannot run." >&2
            echo "JAVA_HOME is set to \"$JAVA_HOME\", but \"\$JAVA_HOME/bin/java\" or \"\$JAVA_HOME/bin/javac\" does not exist." >&2
            return 1
          fi
        fi
      else
        JAVACMD="$(
          'set' +e
          'unset' -f command 2>/dev/null
          'command' -v java
        )" || :
        JAVACCMD="$(
          'set' +e
          'unset' -f command 2>/dev/null
          'command' -v javac
        )" || :

        if [ ! -x "${JAVACMD-}" ] || [ ! -x "${JAVACCMD-}" ]; then
          echo "The java/javac command does not exist in PATH nor is JAVA_HOME set, so mvnw cannot run." >&2
          return 1
        fi
      fi
    }

    # hash string like Java String::hashCode
    hash_string() {
      str="${1:-}" h=0
      while [ -n "$str" ]; do
        char="${str%"${str#?}"}"
        h=$(((h * 31 + $(LC_CTYPE=C printf %d "'$char")) % 4294967296))
        str="${str#?}"
      done
      printf %x\\n $h
    }

    verbose() { :; }
    [ "${MVNW_VERBOSE-}" != true ] || verbose() { printf %s\\n "${1-}"; }

    die() {
      printf %s\\n "$1" >&2
      exit 1
    }

    trim() {
      # MWRAPPER-139:
      #   Trims trailing and leading whitespace, carriage returns, tabs, and linefeeds.
      #   Needed for removing poorly interpreted newline sequences when running in more
      #   exotic environments such as mingw bash on Windows.
      printf "%s" "${1}" | tr -d '[:space:]'
    }

    # parse distributionUrl and optional distributionSha256Sum, requires .mvn/wrapper/maven-wrapper.properties
    while IFS="=" read -r key value; do
      case "${key-}" in
      distributionUrl) distributionUrl=$(trim "${value-}") ;;
      distributionSha256Sum) distributionSha256Sum=$(trim "${value-}") ;;
      esac
    done <"${0%/*}/.mvn/wrapper/maven-wrapper.properties"
    [ -n "${distributionUrl-}" ] || die "cannot read distributionUrl property in ${0%/*}/.mvn/wrapper/maven-wrapper.properties"

    case "${distributionUrl##*/}" in
    maven-mvnd-*bin.*)
      MVN_CMD=mvnd.sh _MVNW_REPO_PATTERN=/maven/mvnd/
      case "${PROCESSOR_ARCHITECTURE-}${PROCESSOR_ARCHITEW6432-}:$(uname -a)" in
      *AMD64:CYGWIN* | *AMD64:MINGW*) distributionPlatform=windows-amd64 ;;
      :Darwin*x86_64) distributionPlatform=darwin-amd64 ;;
      :Darwin*arm64) distributionPlatform=darwin-aarch64 ;;
      :Linux*x86_64*) distributionPlatform=linux-amd64 ;;
      *)
        echo "Cannot detect native platform for mvnd on $(uname)-$(uname -m), use pure java version" >&2
        distributionPlatform=linux-amd64
        ;;
      esac
      distributionUrl="${distributionUrl%-bin.*}-$distributionPlatform.zip"
      ;;
    maven-mvnd-*) MVN_CMD=mvnd.sh _MVNW_REPO_PATTERN=/maven/mvnd/ ;;
    *) MVN_CMD="mvn${0##*/mvnw}" _MVNW_REPO_PATTERN=/org/apache/maven/ ;;
    esac

    # apply MVNW_REPOURL and calculate MAVEN_HOME
    # maven home pattern: ~/.m2/wrapper/dists/{apache-maven-<version>,maven-mvnd-<version>-<platform>}/<hash>
    [ -z "${MVNW_REPOURL-}" ] || distributionUrl="$MVNW_REPOURL$_MVNW_REPO_PATTERN${distributionUrl#*"$_MVNW_REPO_PATTERN"}"
    distributionUrlName="${distributionUrl##*/}"
    distributionUrlNameMain="${distributionUrlName%.*}"
    distributionUrlNameMain="${distributionUrlNameMain%-bin}"
    MAVEN_USER_HOME="${MAVEN_USER_HOME:-${HOME}/.m2}"
    MAVEN_HOME="${MAVEN_USER_HOME}/wrapper/dists/${distributionUrlNameMain-}/$(hash_string "$distributionUrl")"

    exec_maven() {
      unset MVNW_VERBOSE MVNW_USERNAME MVNW_PASSWORD MVNW_REPOURL || :
      exec "$MAVEN_HOME/bin/$MVN_CMD" "$@" || die "cannot exec $MAVEN_HOME/bin/$MVN_CMD"
    }

    if [ -d "$MAVEN_HOME" ]; then
      verbose "found existing MAVEN_HOME at $MAVEN_HOME"
      exec_maven "$@"
    fi

    case "${distributionUrl-}" in
    *?-bin.zip | *?maven-mvnd-?*-?*.zip) ;;
    *) die "distributionUrl is not valid, must match *-bin.zip or maven-mvnd-*.zip, but found '${distributionUrl-}'" ;;
    esac

    # prepare tmp dir
    if TMP_DOWNLOAD_DIR="$(mktemp -d)" && [ -d "$TMP_DOWNLOAD_DIR" ]; then
      clean() { rm -rf -- "$TMP_DOWNLOAD_DIR"; }
      trap clean HUP INT TERM EXIT
    else
      die "cannot create temp dir"
    fi

    mkdir -p -- "${MAVEN_HOME%/*}"

    # Download and Install Apache Maven
    verbose "Couldn't find MAVEN_HOME, downloading and installing it ..."
    verbose "Downloading from: $distributionUrl"
    verbose "Downloading to: $TMP_DOWNLOAD_DIR/$distributionUrlName"

    # select .zip or .tar.gz
    if ! command -v unzip >/dev/null; then
      distributionUrl="${distributionUrl%.zip}.tar.gz"
      distributionUrlName="${distributionUrl##*/}"
    fi

    # verbose opt
    __MVNW_QUIET_WGET=--quiet __MVNW_QUIET_CURL=--silent __MVNW_QUIET_UNZIP=-q __MVNW_QUIET_TAR=''
    [ "${MVNW_VERBOSE-}" != true ] || __MVNW_QUIET_WGET='' __MVNW_QUIET_CURL='' __MVNW_QUIET_UNZIP='' __MVNW_QUIET_TAR=v

    # normalize http auth
    case "${MVNW_PASSWORD:+has-password}" in
    '') MVNW_USERNAME='' MVNW_PASSWORD='' ;;
    has-password) [ -n "${MVNW_USERNAME-}" ] || MVNW_USERNAME='' MVNW_PASSWORD='' ;;
    esac

    if [ -z "${MVNW_USERNAME-}" ] && command -v wget >/dev/null; then
      verbose "Found wget ... using wget"
      wget ${__MVNW_QUIET_WGET:+"$__MVNW_QUIET_WGET"} "$distributionUrl" -O "$TMP_DOWNLOAD_DIR/$distributionUrlName" || die "wget: Failed to fetch $distributionUrl"
    elif [ -z "${MVNW_USERNAME-}" ] && command -v curl >/dev/null; then
      verbose "Found curl ... using curl"
      curl ${__MVNW_QUIET_CURL:+"$__MVNW_QUIET_CURL"} -f -L -o "$TMP_DOWNLOAD_DIR/$distributionUrlName" "$distributionUrl" || die "curl: Failed to fetch $distributionUrl"
    elif set_java_home; then
      verbose "Falling back to use Java to download"
      javaSource="$TMP_DOWNLOAD_DIR/Downloader.java"
      targetZip="$TMP_DOWNLOAD_DIR/$distributionUrlName"
      cat >"$javaSource" <<-END
        public class Downloader extends java.net.Authenticator
        {
          protected java.net.PasswordAuthentication getPasswordAuthentication()
          {
            return new java.net.PasswordAuthentication( System.getenv( "MVNW_USERNAME" ), System.getenv( "MVNW_PASSWORD" ).toCharArray() );
          }
          public static void main( String[] args ) throws Exception
          {
            setDefault( new Downloader() );
            java.nio.file.Files.copy( java.net.URI.create( args[0] ).toURL().openStream(), java.nio.file.Paths.get( args[1] ).toAbsolutePath().normalize() );
          }
        }
        END
      # For Cygwin/MinGW, switch paths to Windows format before running javac and java
      verbose " - Compiling Downloader.java ..."
      "$(native_path "$JAVACCMD")" "$(native_path "$javaSource")" || die "Failed to compile Downloader.java"
      verbose " - Running Downloader.java ..."
      "$(native_path "$JAVACMD")" -cp "$(native_path "$TMP_DOWNLOAD_DIR")" Downloader "$distributionUrl" "$(native_path "$targetZip")"
    fi

    # If specified, validate the SHA-256 sum of the Maven distribution zip file
    if [ -n "${distributionSha256Sum-}" ]; then
      distributionSha256Result=false
      if [ "$MVN_CMD" = mvnd.sh ]; then
        echo "Checksum validation is not supported for maven-mvnd." >&2
        echo "Please disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties." >&2
        exit 1
      elif command -v sha256sum >/dev/null; then
        if echo "$distributionSha256Sum  $TMP_DOWNLOAD_DIR/$distributionUrlName" | sha256sum -c >/dev/null 2>&1; then
          distributionSha256Result=true
        fi
      elif command -v shasum >/dev/null; then
        if echo "$distributionSha256Sum  $TMP_DOWNLOAD_DIR/$distributionUrlName" | shasum -a 256 -c >/dev/null 2>&1; then
          distributionSha256Result=true
        fi
      else
        echo "Checksum validation was requested but neither 'sha256sum' or 'shasum' are available." >&2
        echo "Please install either command, or disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties." >&2
        exit 1
      fi
      if [ $distributionSha256Result = false ]; then
        echo "Error: Failed to validate Maven distribution SHA-256, your Maven distribution might be compromised." >&2
        echo "If you updated your Maven version, you need to update the specified distributionSha256Sum property." >&2
        exit 1
      fi
    fi

    # unzip and move
    if command -v unzip >/dev/null; then
      unzip ${__MVNW_QUIET_UNZIP:+"$__MVNW_QUIET_UNZIP"} "$TMP_DOWNLOAD_DIR/$distributionUrlName" -d "$TMP_DOWNLOAD_DIR" || die "failed to unzip"
    else
      tar xzf${__MVNW_QUIET_TAR:+"$__MVNW_QUIET_TAR"} "$TMP_DOWNLOAD_DIR/$distributionUrlName" -C "$TMP_DOWNLOAD_DIR" || die "failed to untar"
    fi
    printf %s\\n "$distributionUrl" >"$TMP_DOWNLOAD_DIR/$distributionUrlNameMain/mvnw.url"
    mv -- "$TMP_DOWNLOAD_DIR/$distributionUrlNameMain" "$MAVEN_HOME" || [ -d "$MAVEN_HOME" ] || die "fail to move MAVEN_HOME"

    clean || :
    exec_maven "$@"
    
  mvnwCmd: |-
    <# : batch portion
    @REM ----------------------------------------------------------------------------
    @REM Licensed to the Apache Software Foundation (ASF) under one
    @REM or more contributor license agreements.  See the NOTICE file
    @REM distributed with this work for additional information
    @REM regarding copyright ownership.  The ASF licenses this file
    @REM to you under the Apache License, Version 2.0 (the
    @REM "License"); you may not use this file except in compliance
    @REM with the License.  You may obtain a copy of the License at
    @REM
    @REM    http://www.apache.org/licenses/LICENSE-2.0
    @REM
    @REM Unless required by applicable law or agreed to in writing,
    @REM software distributed under the License is distributed on an
    @REM "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    @REM KIND, either express or implied.  See the License for the
    @REM specific language governing permissions and limitations
    @REM under the License.
    @REM ----------------------------------------------------------------------------

    @REM ----------------------------------------------------------------------------
    @REM Apache Maven Wrapper startup batch script, version 3.3.2
    @REM
    @REM Optional ENV vars
    @REM   MVNW_REPOURL - repo url base for downloading maven distribution
    @REM   MVNW_USERNAME/MVNW_PASSWORD - user and password for downloading maven
    @REM   MVNW_VERBOSE - true: enable verbose log; others: silence the output
    @REM ----------------------------------------------------------------------------

    @IF "%__MVNW_ARG0_NAME__%"=="" (SET __MVNW_ARG0_NAME__=%~nx0)
    @SET __MVNW_CMD__=
    @SET __MVNW_ERROR__=
    @SET __MVNW_PSMODULEP_SAVE=%PSModulePath%
    @SET PSModulePath=
    @FOR /F "usebackq tokens=1* delims==" %%A IN (`powershell -noprofile "& {$scriptDir='%~dp0'; $script='%__MVNW_ARG0_NAME__%'; icm -ScriptBlock ([Scriptblock]::Create((Get-Content -Raw '%~f0'))) -NoNewScope}"`) DO @(
      IF "%%A"=="MVN_CMD" (set __MVNW_CMD__=%%B) ELSE IF "%%B"=="" (echo %%A) ELSE (echo %%A=%%B)
    )
    @SET PSModulePath=%__MVNW_PSMODULEP_SAVE%
    @SET __MVNW_PSMODULEP_SAVE=
    @SET __MVNW_ARG0_NAME__=
    @SET MVNW_USERNAME=
    @SET MVNW_PASSWORD=
    @IF NOT "%__MVNW_CMD__%"=="" (%__MVNW_CMD__% %*)
    @echo Cannot start maven from wrapper >&2 && exit /b 1
    @GOTO :EOF
    : end batch / begin powershell #>

    $ErrorActionPreference = "Stop"
    if ($env:MVNW_VERBOSE -eq "true") {
      $VerbosePreference = "Continue"
    }

    # calculate distributionUrl, requires .mvn/wrapper/maven-wrapper.properties
    $distributionUrl = (Get-Content -Raw "$scriptDir/.mvn/wrapper/maven-wrapper.properties" | ConvertFrom-StringData).distributionUrl
    if (!$distributionUrl) {
      Write-Error "cannot read distributionUrl property in $scriptDir/.mvn/wrapper/maven-wrapper.properties"
    }

    switch -wildcard -casesensitive ( $($distributionUrl -replace '^.*/','') ) {
      "maven-mvnd-*" {
        $USE_MVND = $true
        $distributionUrl = $distributionUrl -replace '-bin\.[^.]*$',"-windows-amd64.zip"
        $MVN_CMD = "mvnd.cmd"
        break
      }
      default {
        $USE_MVND = $false
        $MVN_CMD = $script -replace '^mvnw','mvn'
        break
      }
    }

    # apply MVNW_REPOURL and calculate MAVEN_HOME
    # maven home pattern: ~/.m2/wrapper/dists/{apache-maven-<version>,maven-mvnd-<version>-<platform>}/<hash>
    if ($env:MVNW_REPOURL) {
      $MVNW_REPO_PATTERN = if ($USE_MVND) { "/org/apache/maven/" } else { "/maven/mvnd/" }
      $distributionUrl = "$env:MVNW_REPOURL$MVNW_REPO_PATTERN$($distributionUrl -replace '^.*'+$MVNW_REPO_PATTERN,'')"
    }
    $distributionUrlName = $distributionUrl -replace '^.*/',''
    $distributionUrlNameMain = $distributionUrlName -replace '\.[^.]*$','' -replace '-bin$',''
    $MAVEN_HOME_PARENT = "$HOME/.m2/wrapper/dists/$distributionUrlNameMain"
    if ($env:MAVEN_USER_HOME) {
      $MAVEN_HOME_PARENT = "$env:MAVEN_USER_HOME/wrapper/dists/$distributionUrlNameMain"
    }
    $MAVEN_HOME_NAME = ([System.Security.Cryptography.MD5]::Create().ComputeHash([byte[]][char[]]$distributionUrl) | ForEach-Object {$_.ToString("x2")}) -join ''
    $MAVEN_HOME = "$MAVEN_HOME_PARENT/$MAVEN_HOME_NAME"

    if (Test-Path -Path "$MAVEN_HOME" -PathType Container) {
      Write-Verbose "found existing MAVEN_HOME at $MAVEN_HOME"
      Write-Output "MVN_CMD=$MAVEN_HOME/bin/$MVN_CMD"
      exit $?
    }

    if (! $distributionUrlNameMain -or ($distributionUrlName -eq $distributionUrlNameMain)) {
      Write-Error "distributionUrl is not valid, must end with *-bin.zip, but found $distributionUrl"
    }

    # prepare tmp dir
    $TMP_DOWNLOAD_DIR_HOLDER = New-TemporaryFile
    $TMP_DOWNLOAD_DIR = New-Item -Itemtype Directory -Path "$TMP_DOWNLOAD_DIR_HOLDER.dir"
    $TMP_DOWNLOAD_DIR_HOLDER.Delete() | Out-Null
    trap {
      if ($TMP_DOWNLOAD_DIR.Exists) {
        try { Remove-Item $TMP_DOWNLOAD_DIR -Recurse -Force | Out-Null }
        catch { Write-Warning "Cannot remove $TMP_DOWNLOAD_DIR" }
      }
    }

    New-Item -Itemtype Directory -Path "$MAVEN_HOME_PARENT" -Force | Out-Null

    # Download and Install Apache Maven
    Write-Verbose "Couldn't find MAVEN_HOME, downloading and installing it ..."
    Write-Verbose "Downloading from: $distributionUrl"
    Write-Verbose "Downloading to: $TMP_DOWNLOAD_DIR/$distributionUrlName"

    $webclient = New-Object System.Net.WebClient
    if ($env:MVNW_USERNAME -and $env:MVNW_PASSWORD) {
      $webclient.Credentials = New-Object System.Net.NetworkCredential($env:MVNW_USERNAME, $env:MVNW_PASSWORD)
    }
    [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12
    $webclient.DownloadFile($distributionUrl, "$TMP_DOWNLOAD_DIR/$distributionUrlName") | Out-Null

    # If specified, validate the SHA-256 sum of the Maven distribution zip file
    $distributionSha256Sum = (Get-Content -Raw "$scriptDir/.mvn/wrapper/maven-wrapper.properties" | ConvertFrom-StringData).distributionSha256Sum
    if ($distributionSha256Sum) {
      if ($USE_MVND) {
        Write-Error "Checksum validation is not supported for maven-mvnd. `nPlease disable validation by removing 'distributionSha256Sum' from your maven-wrapper.properties."
      }
      Import-Module $PSHOME\Modules\Microsoft.PowerShell.Utility -Function Get-FileHash
      if ((Get-FileHash "$TMP_DOWNLOAD_DIR/$distributionUrlName" -Algorithm SHA256).Hash.ToLower() -ne $distributionSha256Sum) {
        Write-Error "Error: Failed to validate Maven distribution SHA-256, your Maven distribution might be compromised. If you updated your Maven version, you need to update the specified distributionSha256Sum property."
      }
    }

    # unzip and move
    Expand-Archive "$TMP_DOWNLOAD_DIR/$distributionUrlName" -DestinationPath "$TMP_DOWNLOAD_DIR" | Out-Null
    Rename-Item -Path "$TMP_DOWNLOAD_DIR/$distributionUrlNameMain" -NewName $MAVEN_HOME_NAME | Out-Null
    try {
      Move-Item -Path "$TMP_DOWNLOAD_DIR/$MAVEN_HOME_NAME" -Destination $MAVEN_HOME_PARENT | Out-Null
    } catch {
      if (! (Test-Path -Path "$MAVEN_HOME" -PathType Container)) {
        Write-Error "fail to move MAVEN_HOME"
      }
    } finally {
      try { Remove-Item $TMP_DOWNLOAD_DIR -Recurse -Force | Out-Null }
      catch { Write-Warning "Cannot remove $TMP_DOWNLOAD_DIR" }
    }

    Write-Output "MVN_CMD=$MAVEN_HOME/bin/$MVN_CMD"
    
prompts:
  controllerTest: |-    
    @@@freemarker
    @@@prompt
    
    ${cluster.controller}
    
    Given the Java Quarkus Controller class above do:
        1. Generate a Java Quarkus test class to assert most of the content of the Controller as possible.
  controller: |-
    @@@freemarker
    @@@utils.optimizeImports
    @@@prompt@set:cluster.controller@set:endpoints[]
    ```
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.controller;
    
    !!imports!!

    @RestController
    public class ${fileName?replace(".java", "")} {
        !!classContent!!
    }
    ```

    Task:
    As a senior Java engineer, create a RESTful controller method for a Quarkus MVC application that represents a line-by-line translation of the original COBOL source code. 
    Utilize the existing DTOs and Service methods provided. 
    The method should adhere to enterprise-quality standards and be free of placeholders or incomplete code. JavaDoc must be included.
       
    Imports:
    
    USE those imports below and all necessary dependencies and any others required.
    <imports>
    import ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.service.*;
    <imports>
    
    Consider the following list of DTOs: <#list cluster.dtos as _dto>${_dto.name} as ${_dto.file}, </#list>
    
    These DTOs already exist in the project.
    Use one of these DTOs or primitive types as the return type for the method.
    
    Method Creation:
    
    Create a method named <insert method name> {{methodName}}.
    Based on the method name, determine the appropriate RESTful HTTP method/verb (e.g., GET, POST).
    The method should use the equivalent method from the MainService class for its implementation.

    Endpoints Names:

    <#list blueprint as bp>
    <#list bp.paragraphs as bpp>
    <#list bpp.calls as c>
    For microservice ${c.microservice}, program ${c.program} and paragraph ${c.paragraph} use the endpoint name ${c.endpoint}
    </#list>
    </#list>
    </#list>
    
    Service Method Integration:
    
    Use the MainService class to call the implementation.
    Consider the following method in the MainService class as the implementation for this endpoint:
    <code>
    {{baseJavaCode}}
    </code>
    
    Code Generation:
    
    Generate only the method's code suitable for inclusion in a controller class.
    Do not create any classes or main Quarkus classes.
    Ensure the method is correctly annotated and follows RESTful patterns.
    
    RESTful Patterns:
    
    Use appropriate RESTful annotations (e.g., @GetMapping, @PostMapping) based on the determined HTTP verb.
    Ensure the method handles HTTP requests correctly.
    Do not attempt to enhance the code or implement general best practices unless explicitly instructed.
    The goal is to create a faithful representation of the original code in a RESTful context, not to improve upon it.
    JavaDoc inclusion
    I need the detailed JavaDoc to be generated with the ENGLISH text of the entire method. Remember to document the builders.
    Create the JavaDoc header on the first line and define a detailed description of the method.
    Create the JavaDoc for each field of the class
    Code Author is Capco LLM Automation
    
    Naming and Preservation:
    
    Preserve all names of variables and methods as they appear in the original COBOL code and existing Java code.
    Do not add any logic, data manipulation, or functionality that isn't present in the original service method.
    The controller method should act as a thin wrapper around the service method, primarily handling HTTP-specific concerns.
    Unless explicitly specified in the original service method, the controller should return only HTTP status codes, not data objects.
    If the service method doesn't return a value, the controller should return a ResponseEntity<Void>.
    Maintain compatibility for easy interpretation and maintainability.
    
    Comments and Documentation:
    
    Place any necessary text within Java comment blocks.
    Do not include any additional comments or explanations outside of the code.
    
    Output Format:
    
    Provide only the Java code of the method.
    Do not include introductions, explanations, or any additional text.
    Exclude any extra brackets or unnecessary formatting.
    
    Quality Assurance:
    
    Ensure the code is complete and represents a line-by-line translation of the original COBOL source code.
    Do not include placeholders, straw-man code, or skeleton code.
    Double-check the code for any mistakes or errors before finalizing.
    
    Error Handling:
    
    Do not catch generic exceptions; specify the exception types if necessary.
    Do not swallow exceptions; handle them appropriately.
    Avoid returning commented exceptions.
    
    Internal Checklist:
    
    Before finalizing, verify that all instructions have been followed precisely.
    Use an internal checklist to ensure compliance with all steps.
    
    Consistency and Standards:
    
    Follow standard Java coding conventions and best practices.
    Maintain consistent indentation and formatting throughout the code.
    
    No Placeholders:
    
    Ensure the method contains full implementation code.
    Do not include TODO comments or any unimplemented sections.
    
    Final Notes:
    
    Do not create any classes; only the method code should be returned.
    Remember to omit any documentation or explanations in the output.

    Output:

    After all, replace the !!imports!! placeholder with the import statements required to use whole class/methods you created.
    And replace the !!classContent!! with the endpoints and any fields or properties needed for the whole class.
  controllerMethod: |-
  
  dto: |-
    @@@freemarker
    @@@prompt@set:cluster.dtoClasses[]
    [TASK]
    IMPORTANT: You need to figure out how the file is used using the COBOL code you have as input in a way to realize if a PanacheEntity is will be created or just a simple Dto class.
    So, create a JAVA Model or Dto class using the COBOL data storage definition below:
      
    ```
    ${cluster.dtos ? filter(dto -> dto.file == fileName) ? map(dto -> dto.code) ? first}
    ```
    
    [CONSTRAINTS]
      0. Leave a comment at the top of the file with the COBOL ORIGINAL VARIABLE NAME
      1. Use Lombok annotation at the class level to generate the Setters and Getters
      2. Return only the code, nothing more
      3. Don't change the names, only normalize to remove characters different from [a-zA-Z0-9] and use the camelCase approach
      4. The name of the class needs to match the name of the file: ${fileName}
      5. Use the package: ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.dtos
      6. Ensure that the variables take up the smallest memory footprint possible (i.e. use primitive Java types wherever possible), but at the same time do not sacrifice accuracy, particularly where decimals are involved
      7. Ensure to solve any placeholders or TODO comments
      8. No syntax errors or logical mistakes
      9. Remember to include all the import statements needed within the class
      10. Whenever I use ${fileName} after this, remember the content you returned here as the value of this placeholder
      11. Include the necessary annotations for Quarkus Data ONLY WHEN the Dto relates to a Data Storage you already classified as one of which needs to be persisted/attached to a file/database.
  client: |-
    @@@skip(${#blueprint[#cluster['index']]['paragraphs'].?[!#this['calls'].isEmpty()].isEmpty()})
    @@@freemarker
    @@@prompt
    Generate a RestClient interface for endpoints below:
    
    <#list blueprint[cluster.index].paragraphs as paragraph>
       ${paragaph.calls?map(bp -> 'For CALL ' + bp.paragraph + ' use the microservice ' + bp.microservice + ' endpoints ' + bp.endpoint + ' [CODE_REFERENCE]\n' + bp.code + '\n[/CODE_REFERENCE]')?join("\n")}
    </#list>

    Also be aware that already exist controllers with some endpoints for other microservices. They are listed below. Please use the same name and signature whenever you find a match!
    <#if endpoints?has_content>
    ${endpoints?join("\n\n\n\n")}
    </#if>
    
    [CONSTRAINTS]
      0. The class/interface name needs to be: ${fileName}
      1. Consider the equivalency map of Storage Name and Java Dto below:
        ```
        <#list cluster.dtos as _dto>
          ${_dto.name} is the same of ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.dtos.${_dto.file?replace(".java", "")} with the COBOL code below:
          [DTO_CODE]
          ${_dto.code}
          [/DTO_CODE]
        </#list>
        ```
      2. Do not include straw-man code, skeleton code, or any placeholders.
      3. Do not leave any methods without implementation or with TODO comments.
      4. Include any import needed.
      5. Use a standard class/primitive type as return type or any of the DTOs listed.
      6. Return only the code, nothing more
  serviceMethod: |-
    [TASK]
    Create a JAVA method using the COBOL paragraph content below as reference:
    
    ```
    !!paragraphCode!!
    ```

    Consider the map of endpoints below as a guide to select the right client/method of client class you justed generated:

    ```
    !!endpoints!!
    ```
    
    [CONSTRAINTS]
      1. Consider the equivalency map of Storage Name and Java Dto below:
        [CONTEXT]
        <#if cluster.dtoClasses?has_content>
        <#list cluster.dtoClasses as _dto>
          [DTO_CODE]
          ${_dto}
          [/DTO_CODE]
        </#list>
        </#if>
        
        <#if cluster.model?hastContent>
          [MODEL_CODE]
          ${cluster.model}
          [/MODEL_CODE]
          
          [LOCAL_VARIABLES]
          <#if cluster.locals?hasContent>
          <#list cluster.locals as local>
          [VARIABLE]
          ${local.code}
          [/VARIABLE]
          </#list>
          </#if>
          [/LOCAL_VARIABLES]
        </#if>
        
        <#if cluster.exceptions?hasContent>
        [EXCEPTIONS_CLASS_REFERENCE]
        ${cluster.exceptions}
        [/EXCEPTIONS_CLASS_REFERENCE]
        </#if>
        
        [/CONTEXT]
        
      2. Repositories you can use:        
      3. NEVER generate incomplete code, like including TODOs or similar placeholders, ALWAYS return complete methods!!!
      4. Do not include staw-man code, skeleton code, or any placeholders.
      5. Do not leave any methods without implementation or with TODO comments.
      6. Encapsulate all code within a try-catch block.
      7. Include a JavaDoc to the method.
      8. Use a standard class/primitive type as return type or any of the DTOs listed.
      9. Return only the code, in one single block, nothing more
      10. You are not allowed to generate methods with more than 30 lines of code, whenever it gets over that size split the method in compreensive pieces without touching the original method 
      signature that you are refactoring. Any of the additional methods needs to be declared as private.
      11. Remove any comments inside the method! They are not allowed!
      12. Every method that is supposed to change a value from another scope needs to return a value or save it on the respective repository/file already been used by the service.
      13. Consider that already exist an @Slf4j log annotation on the class that the method will be included, so you can address any logging message using log instance
  service: |-
    @@@freemarker
    @@@evalEachBlock
    @@@utils.optimizeImports
    @@@prompt@set:cluster.service
    Improve the class below, but ensure to not omit any code and keep using @Slf4j and the Custom Exceptions classes that already exist. Some of the dependencies imports still missing, please include them considering it a Spring Boot 3 application.
    Be clever, some methods doesn't makes sense to exist on Java behaviour (like the common EXIT paragraphs from cobol that doesn't do anything).   
    
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.service;
    
    import ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.exception.*;
    import ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.model.*;
    import ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.repository.*;
    
    !!imports!!
    
    @Slf4j
    @Service
    public class ${fileName?replace(".java", "")} {
      <#list blueprint[cluster.index].paragraphs as paragraph>
        <#if !paragraph.name?contains("ABEND") && !paragraph.name?contains("EXIT")>
        <#noparse>
        @@@{
        @@@freemarker
        @@@newPrompt
        @@@repromptAndReplacePoorMethods
        @@@utils.extractMarkdownCode
        @@@withoutclass
        <#assign blueprintParagraph = blueprint[cluster.index].paragraphs?filter(p -> p.name = </#noparse>"${paragraph.name}"<#noparse>)?first>
        ${recipe.prompts.serviceMethod
            ?replace('!!paragraphCode!!', blueprintParagraph.code)
            ?replace('!!endpoints!!', blueprintParagraph.calls?map(bp -> 'For CALL ' + bp.paragraph + ' use the microservice ' + bp.microservice + ' endpoint ' + bp.endpoint)?join("\n"))
        }
        }@@@
        </#noparse>
        </#if>
      </#list>
    }
  service2: |-
    @@@freemarker
    @@@prompt@set:cluster.service
    [COBOL_PARAGRAPHS]
    ${recipe.vars.cobol}
    [/COBOL_PARAGRAPHS]
    
    [EXCEPTION_CLASSES]
    ${cluster.exceptions}
    [/EXCEPTION_CLASSES]
    
    [REPOSITORY]
    <#list cluster.repositories as repo>
    ${repo}
    </#list>
    [/REPOSITORY]
    
    [MODEL]
    ${cluster.model}
    [/MODEL]
    
    Based on the COBOL_PARAGRAPHS 1000-MAIN-PROCESSING, 1000-SELECT-MAX, 2001-UPDATE-RTN translate those logics on a JAVA Spring Boot 3 Service class taking advantage of the [EXCEPTION_CLASSES], [REPOSITORY] and [MODEL].
    
    If you translate a paragraph that process some file, consider the equivalent Java method to receive the content of the file as a Spring parameter.
    
    Use the best practices of a Spring Boot 3 solution for that service class. Ensure to use @Slf4j logging schema, and @Lombok annotations if needed.
    The class should be named MainService.
    
    Don't include anyother content, just the single class code I asked you for.
    Set local variables to any method that do some file reading based on the equivalent File Section and File Definition of the original COBOL code.
  serviceTest: |-
    @@@freemarker
    @@@prompt
    @@@utils.optimizeImports
    
    ${cluster.service}
    
    Given the Java Spring Service class above do:
        1. Generate a Java Spring Test Class to assert the most of the content of the Service as possible.
        2. Return only the content of the class file, nothing more.
        3. Ensure to use the best Jupiter + Spring Boot Testing practices.
  taskletService: |-
    @@@freemarker
    @@@newPrompt@set:cluster.taskletService
    [SERVICE]
    ${cluster.service}
    [/SERVICE]
    
    Create the methods below:
    <#list jclProfile?filter(jclStep -> jclStep.isProgramCall == true) as jclStep>
    Method name: ${jclStep.stepName} - Parameter1: fileName (String) - Parameter2: contribution (StepContribution) - Parameter3: chunkContext (ChunkContext)
    </#list>
    
    Based on this Spring Boot Service create a TaskletService with the name TaskeletService as well.
    Please only take care of the TaskletService class and ensure to create the whole content without comments and without omitting any part of the needed code.
    Use @Slf4j whenever it needs to log something.
    
    1. Pure class, without any interface implementation.
    2. Generate just the methods you are asked for, nothing more.
    3. Follow the best practices of a tasklet method, like returning a RepeatStatus value.
    4. Connect the right method with the equivalent SERVICE method that expect to receive a fileContent as input.
    5. Every fileNames represent a file inside the resources folder, so implement the code needed to read the content whenever is needed.
    
  batchConfig: |-
    @@@freemarker@set:cluster.batchConfig
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.config;
    
    import org.springframework.batch.core.Job;
    import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;
    import org.springframework.batch.core.launch.JobLauncher;
    import org.springframework.batch.core.launch.support.TaskExecutorJobLauncher;
    import org.springframework.batch.core.repository.JobRepository;
    import org.springframework.batch.core.repository.support.JobRepositoryFactoryBean;
    import org.springframework.beans.factory.annotation.Qualifier;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    import org.springframework.core.task.SimpleAsyncTaskExecutor;
    import org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseBuilder;
    import org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseType;
    import org.springframework.jdbc.support.JdbcTransactionManager;
    
    import javax.sql.Datasource;
    
    @Configuration
    @EnableBatchProcessing
    public class BatchConfig {
        @Bean(name = "batchDataSource")
        public DataSource dataSource() {
            return new EmbeddedDatabaseBuilder().setType(EmbeddedDatabaseType.HSQL)
                        .addScript("/org/springframework/batch/core/schema-hsqldb.sql")
                        .generateUniqueName(true)
                        .build();
        }
        
        @Bean(name = "batchTransactionManager")
        public JdbcTransactionManager transactionManager(@Qualifier("batchDataSource") DataSource dataSource) {
            return new JdbcTransactionManager(dataSource);
        }
        
        @Bean
        public JobRepository jobRepository(@Qualifier("batchDataSource") DataSource dataSource, @Qualifier("batchTransactionManager") JdbcTransactionManager transactionManager) throws Exception {
            JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean();
            factory.setDataSource(dataSource);
            factory.setTransactionManager(transactionManager);
            factory.afterPropertiesSet();
            
            return factory.getObject();
        }
        
        @Bean(name = "asyncJobLauncher")
        public JobLauncher asyncJobLauncher(JobRepository jobRepository, Job job) throws Exception {
            TaskExecutorJobLauncher JobLauncher = new TaskExecutorJobLauncher();
            jobLauncher.setJobRepository(jobRepository);
            jobLauncher.setTaskExecutor(new SympleAsyncTaskExecutor());
            jobLauncher.afterPropertiesSet();
            
            return jobLauncher;
        }
    }
  batchStepsConfig2: |-
    @@@freemarker
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.config;
    
    import ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.service.TaskletService;    
    import org.springframework.batch.core.Job;
    import org.springframework.batch.core.Step;
    import org.springframework.batch.core.configuration.annotation.JobScope;
    import org.springframework.batch.core.configuration.annotation.EnableBatchProcessing;
    import org.springframework.batch.core.job.builder.JobBuilder;
    import org.springframework.batch.core.repository.JobRepository;
    import org.springframework.batch.core.step.builder.StepBuilder;
    import org.springframework.batch.repeat.RepeatStatus;
    import org.springframework.beans.factory.annotation.Qualifier;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    import org.springframework.jdbc.support.JdbcTransactionManager;
    
    import lombok.extern.slf4j.Slf4j;
    import lombok.RequiredArgsConstructor;
    
    @Configuration
    @EnableBatchProcessing
    @Slf4j
    @RequiredArgsConstructor
    public class BatchStepsConfig {
        private final TaskletService taskletService;
        
        @Bean
        public Job job(JobRepository jobRepository, Step ${jclProfile?filter(jclStep -> jclStep.execTarget != "PWJCABND")?map(jclStep -> jclStep.stepName)?join(", Step ")}) {
            return new JobBuilder("mainJob", jobRepository)
                    .start(${jclProfile[0].stepName})
                    <#list jclProfile?filter(jclStep -> jclStep.execTarget != "PWJCABND") as jclStep>
                    <#if (jclStep?index > 0>
                    .next(${jclStep.stepName})
                    </#if>
                    </#list>
                    .build();
        }
        
        <#list jclProfile?filter(jclStep -> jclStep.execTarget != "PWJCABND") as jclStep>
        @Bean
        @JobScope
        Step ${jclStep.stepName}(JobRepository jobRepository, @Qualifier("batchTransactionManager") JdbcTransactionManager JdbcTransactionManager) {
            return new StepBuilder("${jclStep.name}", jobRepository)
                    .tasklet((contribution, chunkContext) -> {
                        log.info("Executing ${jclStep.stepName}");
                        <#if jclStep.isProgramCall == true>
                        TaskletService.${jclStep.stepName}("${SuperUtils.WHD(cluster.stepsInputs)?filter(it -> it.stepName == jclStep.stepName)?map(it -> it.inputFile)?first}", contribution, chunkContext)
                        </#if>
                        
                        return RepeatStatus.FINISHED;
                    }, jdbcTransactionManager)
                    .build();
        }
        </#list>
    }
  batchStepsConfig: |-
    @@@freemarker
    @@@newPrompt@set:cluster.batchStepsConfig
    [JCL_PROFILE]
    ${recipe.vars.jclProfile}
    [/JCL_PROFILE]
    
    The tasklet for executing the COBOL Program is:
    [COBOL_PROGRAM_TASKLET]
    ${cluster.taskletService}
    [/COBOL_PROGRAM_TASKLET]
    
    [BATCH_CONFIG]
    ${cluster.batchConfig}
    [/BATCH_CONFIG]
    
    Any other tasklet needed please ensure to create as well on this file but in that case without any logic, just a true return to keep the process flowing and simulate the non-functional steps of the Job.
    
    Based on the JSON that describes the JCL steps on the same order they appear, generate a Batch Steps Config file following the rules below:
    0. ClassName: BatchStepsConfig
    1. Don't rely on old JobBuilderFactory and StepBuilderFactory, instead take advantage of the beans below:
        - DataSource dataSource
        - JdbcTransactionManager transactionManager
        - JobLauncher jobLauncher
    2. Package package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.config;
    3. The tasklets should implement the same of the COBOL program but in Java.
    4. Name the steps accordingly with the JCL, change only the name of the step that triggers the execution of COBOL program (don't forget that it can be handled sometimes inderectly from a Control Card reference used as input for the step). This step that calls the COBOL Program please name as "execute".
    5. Retrieve only the content of the BatchConfig class, don't include anything else on your answer.
    6. Create a mockup result on every step that isn't related to a Cobol Program call like those which perform some utilities (sort, IEFBR14, DYL280)
    7. Use @Slf4j whenever it needs to log something.
    8. Step template to be used:
    @Bean
    @JobScope
    public Step stepName(JobRepository jobRepository, JdbcTransactionManager jdbcTransactionManager) {
        return new StepBuilder("stepName", jobRepository)
                .tasklet(someTaskletFunction(), jdbcTransactionManager)
                .build();
    }
    
    IMPORTANT: ignore the steps that execute the PWJCABND program.
  batchController:
    @@@freemarker
    @@@newPrompt@set:cluster.controller
    Consider the Spring Batch Job Configuration as reference for the values/jobs/parameters available to deal with.
    ${cluster.batchConfig}
    
    and batchStepsConfig also
    
    ${cluster.batchStepsConfig}
    
    Consider the JCL/CTC contents below as a reference to figure out which parameters are needed to execute the job. Make sure to consider only the step that triggers the Cobol Program,
    all other steps that uses utilities like IEFBR14, SORT, DYL280 can be totally disconsidered.
    
    Create a JAVA Spring Controller class to expose an endpoint /api/v1/startJob that will trigger the execution of a Spring Batch Job, receiving the same parameters that are used on the step that calls the Cobol Program
    0. ClassName: batchController
    1. Package package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.controller;
    2. Expose an endpoint that will start the JOB with the parameters needed (you can have a look inside the JCL/PRC/CTC files to list the dependencies)
    3. Retrieve only the content for the BatchController.java class, don't include anything else on your answer.
    4. Use @@Slf4j whenever it needs to log something.
  exception: |-
    @@@freemarker
    @@@newPrompt@set:cluster.exceptions
    [JCL_CONTEXT]
    ${recipe.vars.job}
    [/JCL_CONTEXT]
    
    [COBOL_CONTEXT]
    <#list blueprint[cluster.index].paragraphs as bp>
    [PARAGRAPH: ${bp.name}]
    ${bp.code}
    [/PARAGRAPH: ${bp.name}]
    </#list>
    [/COBOL_CONTEXT]
    
    Generate a Java Class that will include the Global Exception Handler from Spring Boot and as inner classes one class for each ABEND (Abnoumous End) or Error Handling needs.
    The JCL_CONTEXT is your reference to figure out which exception classes needs to be generated for the BatchConfig usage, prefix the name of those exceptions with job, like "jobFileException".
    The COBOL_CONTEXT is your reference to figure out which exception classes needs to be generated for any further Business Service class, prefix them with "business", like "businessUpdatingException".
    0. ClassName: ExceptionsHandler
    1. Package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.exception;
    2. Use @Slf4j whenever it needs to log something.
    3. This is not for a MVC application, it will be used by some Tasklets of a Spring Batch job, so don't rely on ResponseEntity returns.
  model: |-
    @@@freemarker
    @@@newPrompt@set:cluster.model
    @@@utils.extractMarkdownCode
    @@@saveFirstFieldAnnotatedWith(Id, cluster.repository.IdType)
    @@@evalAndWriteFile(${'recipe.prompts.repository,' + #fileFolder.replace('/model', '/repository') + '/' + #fileNameWithoutExtension + 'Repository.java'})
    [CONTEXT]
    ${recipe.vars.db2}
    [/CONTEXT]
    
    Generate a Java Class following Spring Boot Data best practices for creating an @Entity file.
    0. ClassName: Model.java
    1. Package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.model;
    2. Retrieve ONLY the content of the Model.java class, nothing more!
    3. Use Lombok annotations to let the class be concise without being incomplete!
    4. If none of the primary key is defined on the DB2 structured, include an adittional column called ID with a @GeneratedValue annotation, otherwise just map the field to the same primary key column.
    5. Use import jakarta.persistence.* instead of javax.persistence.*
  repository: |-
    @@@freemarker@set:cluster.repositories[]
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.repository;
    
    import ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.model.${fileNameWithoutExtension};
    import org.springframework.data.jpa.repository.JpaRepository;
    
    import java.sql.Date;
    
    public interface ${fileNameWithoutExtension}Repository extends JpaRepository<${fileNameWithoutExtension}, ${cluster.repository.IdType}> {
    }
  shellCommands:
    @@@freemarker
    @@@newPrompt
    Based on the [BATCH_CONFIG] below, which exposes a Spring Job to be used, create a full implementation of a Spring Shell Command to start the Job Execution. Don't forget to take care of any parameters as well.
    
    [BATCH_CONFIG]
    ${cluster.batchConfig}
    [/BATCH_CONFIG]
    
    [BATCH_STEPS_CONFIG]
    ${cluster.batchStepsConfig}
    [/BATCH_STEPS_CONFIG]
    
    [CONSTRAINTS]
    0. Package of the file: ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.shell;
    1. Generate just one class with the name ShellCommands.java
    2. Include the whole of that class, don't rely on placeholders and NEVER omit any piece of code.
    3. Ensure to use the @Slf4j as part of the solution.
  dataSourceConfig: |-
    @@@freemarker
    package ${recipe.vars.groupId + '.' + cluster.clusterNormalizedName}.config;
    
    import com.zaxxer.hikari.HikariDataSource;
    import jakarta.persistence.EntityManagerFactory;
    import org.springframework.context.annotation.Bean;
    import org.springframework.context.annotation.Configuration;
    import org.springframework.context.annotation.Primary;
    import org.springframework.orm.jpa.JpaTransactionManager;
    import org.springframework.transaction.annotation.EnableTransactionManagement;
    
    import javax.sql.DataSource;
    
    @configuration
    public class DataSourceConfig {
        @Primary
        @Bean(name = "dataSource")
        public DataSource h2DataSource() {
            HikariDataSource dataSource = new HikariDataSource();
            dataSource.setDriverClassName("org.h2.Driver");
            datasource.setJdbcUrl("jdbc:h2:mem:test");
            dataSource.setUsername("sa");
            dataSource.setPassword("password");
            return dataSource;
        }
    
        @Primary
        @Bean(name = "transactionManager")
        public JpaTransactionManager transactionManager(EntityManagerFactory entityManagerFactory) {
            return new JpaTransactionManager(entityManagerFactory);
        }
    }
  stepsInputs: |-
    @@@freemarker
    @@@prompt@set:cluster.stepsInputs
    
    Considering the JCL defineds between the [CONTEXT] and [/CONTEXT] tags, replace any JCL variable that you can find inside between the [TEMPLATE] and [/TEMPLATE] tags by the right value from any of the JCL content that I gave you.
    Retrieve just the result of the new content, nothing more.
    
    [CONTEXT]
    ${recipe.vars.jobs}
    [/CONTEXT]
    
    [TEMPLATE]
    [
    <#list jclProfile?filter(it -> it.isProgramCall == true) as jclStep>
        {
            "stepName": "${jclStep.stepName}",
    <#assign detailin = jclStep.parameters?filter(it -> it?starts_with("DETAILIN "))?first>
    <#if detailin?has_content>
    <#assign match = detailin?matches(r".*?DSN=([^,\s(]+).*")>
    <#if match>
            "inputFile": "${match?groups[1]}"
    <#else>
            "inputFile": null
    </#if>
    </#if>
        }
    </#list>
    ]
    [/TEMPLATE]
  classDiagram: |-
    @@@prompt
    @@@utils.extractMarkdownCode
    @@@plantuml(${#rootFolder + '/' + #cluster['clusterNormalizedName'] + '/' + #filePath})
    Giving the Java classes I shared with you, and considering this is a Spring Boot Application, do:
    
    0. Consider that some of the dependencies between classes can be injected during the runtime, in that case using Spring Beans.
    1. Generate a class diagram for the whole content, and wire the classes to show the dependencies between them.
    2. Ensure to distribute the nodes on a way that the connections and labels are clearly visible and well suited.
    3. Explore more vertically the size instead of expanding too much horizontally the diagram.
    4. Make sure that PlantUML syntax is right an the script could be used to generate an image without any problems.
    
    Output format:
    - Return only the PlantUML diagram/script, don't include nothing more than this on your answer.
  flowDiagram: |- # Remove the assumption to be using chat mode, for that we need to do it incrementally with freemarker and batch size of classes to be called each time, and someway to recall the dependencies left to be connected on the last part.
    @@@prompt
    @@@utils.extractMarkdownCode
    @@@plantuml(${#rootFolder + '/' + #cluster['clusterNormalizedName'] + '/' + #filePath})
    Giving the Java classes I shared with you, and considering this is a Spring Boot Application, do:
    
    0. Consider that some of the dependencies between classes can be injected during the runtime, in that case using Spring Beans.
    1. Understand the way each class interact to each other on the project.
    2. Generate a flow that first comprehends the configuration of the tool during the startup that stop on  a "user interaction" from that you expand other paths based on the existent ways to start the job/controller/shell commands or whatever entrypoints for the business logic that exists on the application.
    3. Format that flow using PlantUML syntax, don't include parameters of methods on the details, neither rely on class definitions, just need to connect the classes between each other using a concise but meaningful description for the label of each connection for the purpose or reason why they are being wired up.
    4. Ensure to retrieve the whole script without placeholders or anything left to be generated.
    5. Ensure to distribute the nodes on a way that the connections and labels are clearly visible and well suited.
    
    Output format:
    - Return only the PlantUML diagram/script, don't include nothing more than this on your answer.
  fakeDiagram: |-
    @@@prompt
    @@@utils.extractMarkdownCode
    @@@plantuml(${#rootFolder + '/' + #cluster['clusterNormalizedName'] + '/' + #filePath})
    Generate a PlantUML content to the Human common lifecyle.
    Return only the PlantUML script, don't include anything more.
transforms:
  newPrompt: |-
    import com.capco.brsp.synthesisengine.service.ITransform
    import com.capco.brsp.synthesisengine.service.ScriptService
    import org.springframework.context.ApplicationContext
    
    class NewPromptTransform implements ITransform {
        @Override
        String execute(ApplicationContext applicationContext, Map<String, Object> projectContext, String content, String transformParams) {
            def superService = applicationContext.getBean(SuperService.class)
            def tokens = (content =~/w+[\\p{P}\\p{S}]/).size()
            System.out.println("Prompt size: ${content.length()} - approx. tokens: ${tokens} - piece: ${content.take(500).replace('\n', '\\n')}${content.length() > 500 ? '...' : ''}")
            def newContent = superService.BCF(projectContext, content)
            
            return newContent
        }
    }
  evalAndWriteFile: |-
    import com.capco.brsp.synthesisengine.service.ITransform
    import com.capco.brsp.synthesisengine.service.ScriptService
    import org.springframework.context.ApplicationContext
    import com.jayway.jsonpath.JsonPath
    import com.jayway.jsonpath.PathNotFoundException
    import com.capco.brsp.synthesisengine.utils.ConcurrentLinkedHashMap
    import com.capco.brsp.synthesisengine.utils.ConcurrentLinkedList
    import com.capco.brsp.synthesisengine.utils.SuperUtils
    
    import java.nio.file.Paths
    
    class EvalAndWriteFileTransform implements ITransform {
        Object anyCollectionGet(Object target, String path) {
            try {
                return JsonPath.read(target, path)
            } catch (PathNotFoundException pnfe) {
                return null
            }
        }
        
        @Override
        String execute(ApplicationContext applicationContext, Map<String, Object> projectContext, String content, String transformParams) {
            def scriptService = applicationContext.getBean(ScriptService.class)
            def solvedParams = scriptService.autoEval(transformParams, new ConcurrentLinkedList<>())
            def templateReference - solvedParams.split(',')[0]
            def evaluatedPath - solvedParams.split(',')[1]
            
            def relativeFilePath = FileUtils.pathJoin(projectContext.cluster.clusterNormalizedName, evaluatedPath).toString().trim()
            def fullFilePath = FileUtils.absolutePathJoin(projectContext.rootFolder, relativeFilePath)
            
            def filesMetaData = projectContext.files_metadata
            if (filesMetaData == null) {
                filesMetaData = new ConcurrentLinkedHashMap<>()
                projectContext.files_metadata = filesMetaData
            }
            
            def tempFile = filesMetaData.get(relativeFilePath)
            if (tempFile == null) {
                tempFile = new ConcurrentLinkedHashMap<>()
                filesMetaData.put(relativeFilePath, tempFile)
            }
            
            def tempHistory = tempFile.history
            if (tempHistory == null) {
                tempHistory = new ConcurrentLinkedList<String>()
                tempFile.history = tempHistory
            }
            
            String templateContent = anyCollectionGet(projectContext, templateReference) as String
            String evaluatedContent = scriptService.autoEval(templateContent, tempHistory)
            
            SuperUtils.WGB(fullFilePath, evaluatedContent, false)
            
            return content
        }
    }
  regexReplace: |-
    import com.capco.brsp.synthesisengine.service.ITransform
    import org.springframework.context.ApplicationContext
    
    class RegexReplaceTransform implements ITransform {
        def replaceUsingRegex(originalContent, placeholder, sourceContentForRegex, regexPattern, regexReplacer) {
            def pattern = ~regexPattern
            def matcher = (sourceContentForRegex =~ pattern)
            if (matcher.find()) {
                def result = originalContent.replace(placeholder, matcher.groupCount() > 0 ? matcher.replaceAll(regexReplacer) : placeholder)
                return result
            }
            return originalContent
        }
        
        @Override
        String execute(ApplicationContext applicationContext, Map<String, Object> projectContext, String content, String transformParams) {
            System.out.println("Trying to regexReplace")
            def matcher = content =~ /"([^"\\]*(\\.[^"\\]*)*)"/
            if (matcher) {
                def placeholder = matcher[0][1]
                def contentSource = matcher[1][1]
                def regexPattern = matcher[2][1]
                def regexReplacer = matcher[3][1]
                return replaceUsingRegex(content, placeholder, contentSource, regexPattern, regexReplacer)
            }
            System.out.println("Failed to match params on the RegexReplaceTransform")
            
            return content
        }
    }
  saveFirstFieldAnnotatedWith: |-
    import com.capco.brsp.synthesisengine.service.ITransform
    import org.springframework.context.ApplicationContext
    import com.github.javaparser.JavaParser
    import com.github.javaparser.ast.CompilationUnit
    import com.github.javaparser.ast.body.FieldDeclaration
    import com.capco.brsp.synthesisengine.utils.OLDUtils
    
    class SaveFirstFieldAnnotatedWithTransform implements ITransform {
        @Override
        String execute(ApplicationContext applicationContext, Map<String, Object> projectContext, String content, String transformParams) {
            try {
                def annotation = transformParams.split(",")[0].trim()
                def path = transformParams.split(",")[1].trim()
                
                System.out.println("Annotation: ${annotation} - Path: ${path}".toString())
                
                String fieldType = "Object"
                try {
                    JavaParser javaParser = new JavaParser()
                    CompilationUnit cu = javaParser.parse(content).getResult().get()
                    FieldDeclaration fieldDeclaration = cu.findAll(FieldDeclaration.class).stream()
                        .filter(field -> field.getAnnotations().stream().anyMatch(ann -> ann.getNameAsString().equalsIgnoreCase(annotation)))
                        .findFirst().orElse(null)
                    fieldType = fieldDeclaration.getVariable(0).getType()
                } finally {
                    Utils.b(projectContext, path, fieldType)
                }
            } finally {
                return content
            }
        }
    }
  renameFileAsClass: |-
    import com.capco.brsp.synthesisengine.service.ITransform
    import org.springframework.context.ApplicationContext
    import com.github.javaparser.JavaParser
    import com.github.javaparser.ast.CompilationUnit
    import com.github.javaparser.ast.body.ClassOrInterfaceDeclaration
    import com.github.javaparser.ast.body.FieldDeclaration
    import com.capco.brsp.synthesisengine.utils.OLDUtils
    import com.capco.brsp.synthesisengine.utils.FileUtils
    import java.io.File
    
    class RenameFileAsClassTransform implements ITransform {
        @Override
        String execute(ApplicationContext applicationContext, Map<String, Object> projectContext, String content, String transformParams) {
            try {
                def filePath = transformParams.trim()
                def file = new File(filePath)
                def fileName = file.name
                def extension = fileName.tokenize('.').last()
                
                System.out.println("File Path: ${filePath} - File Name: ${fileName} - Extension: ${extension}".toString())
                
                try {
                    JavaParser javaParser = new JavaParser()
                    CompilationUnit cu = javaParser.parse(content).getResult().get()
                    ClassOrInterfaceDeclaration classDeclaration = cu.findAll(ClassOrInterfaceDeclaration.class).stream().findFirst().orElse(null)
                    String oldFileName = "${fileName}.${extension}"
                    String newFileName = "${classDeclaration.getName().getIdentifier()}.${extension}"
                    String newFilePath = filePath.replace(oldFileName, newFileName)
                    
                    var oldRelativeFilePath = FileUtils.pathJoin(cluster.clusterNormalizedName, filePath)
                    var oldFullFilePath = FileUtils.absolutePathJoin(context.rootFolder, oldRelativeFilePath)
                    
                    var newRelativeFilePath = FileUtils.pathJoin(cluster.clusterNormalizedName, newFilePath)
                    var newFullFilePath = FileUtils.absolutePathJoin(context.rootFolder, newRelativeFilePath)
                    FileUtils.writeFile(newFullFilePath, content, false)
                } catch (Exception ignored) {}
            } finally {
                return content
            }
        }
    }
files:
    tables: |-
        [
            {
                "table": {
                    "uid": 3097,
                    "name": "ANYTABLEDB2",
                    "file_path": "/app/ANYTABLEDB2.db2",
                    "raw_code": "NULL"
                }
            }
        ]